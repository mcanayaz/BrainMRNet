{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QaFrvOWeGWoE",
    "outputId": "adc13e31-0656-4629-8a71-b7e76940fdab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from albumentations import *\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model, Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  
  
 
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uGT_8VyTZt2T"
   },
   "outputs": [],
   "source": [
    "class DATASET:\n",
    "    \n",
    "    \"\"\"\n",
    "    input_shape           --> TUPLE.wanted image size\n",
    "    batch_size            --> INT.yielding data size for every iteration\n",
    "    orders                --> LIST.which images will be used. max=len(all_images). it can be used for K-fold(CV).\n",
    "    base_dir              --> STR.the DIR which is include images.\n",
    "    seed                  --> INT. This allow to dataset generator to more reproduciable and it ensures that x and y are shuffled with compatible.\n",
    "    augment               --> BOOL. Augment data or not.\n",
    "    train_test_ratio      --> How much of data will be used as test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, batch_size, orders, base_dir, seed, train_test_ratio, augment=True):\n",
    "        self.SHAPE                 = input_shape\n",
    "        self.BATCH_SIZE            = batch_size\n",
    "        self.arr                   = orders\n",
    "        self.SEED                  = seed\n",
    "        self.TT_RATIO              = train_test_ratio\n",
    "        self.AUG                   = augment\n",
    "        \n",
    "        self.BASE_DIR              = base_dir\n",
    "        \n",
    "        \n",
    "    def get_paths_n_labels(self):\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.get_paths_n_labels()[0])\n",
    "    \n",
    "    def get_img(self, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        return np.array(img)\n",
    "    \n",
    "    def augmenting(self, img):\n",
    "        if self.AUG:\n",
    "            augment = Compose([VerticalFlip(p=0.5),\n",
    "                               HorizontalFlip(p=0.5),\n",
    "                               RandomBrightnessContrast(p=0.3),\n",
    "                               ShiftScaleRotate(p=0.5, shift_limit=0.0, scale_limit=0.05, rotate_limit=20)])  \n",
    "        else:\n",
    "            augment = Compose([])  \n",
    "\n",
    "        img = augment(image=img)['image']\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def resize_and_normalize(self, img):\n",
    "        img = resize(img, self.SHAPE)\n",
    "        return img\n",
    "    \n",
    "    def get_shuffled_data(self):\n",
    "        img_paths, labels = self.get_paths_n_labels()\n",
    "\n",
    "        np.random.seed(self.SEED) \n",
    "        np.random.shuffle(img_paths)\n",
    "        \n",
    "        np.random.seed(self.SEED) \n",
    "        np.random.shuffle(labels)\n",
    "        \n",
    "        return img_paths, labels\n",
    "        \n",
    "    def split_train_test(self, get):  # get=={\"train\",\"test\"}\n",
    "        img_paths, labels = self.get_shuffled_data()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(img_paths, labels, test_size=self.TT_RATIO, random_state=self.SEED)\n",
    "        \n",
    "        if get=='train':\n",
    "            return x_train, y_train\n",
    "        \n",
    "        elif get=='test':\n",
    "            return x_test, y_test\n",
    "    \n",
    "    def data_generator(self):\n",
    "        img_paths, labels = self.split_train_test(get=\"train\")\n",
    "        \n",
    "        while True:\n",
    "            x = np.empty((self.BATCH_SIZE,)+self.SHAPE, dtype=np.float32)\n",
    "            y = np.empty((self.BATCH_SIZE, 2), dtype=np.float32)\n",
    "\n",
    "            batch = np.random.choice(self.arr, self.BATCH_SIZE)\n",
    "\n",
    "            for ix, id_ in enumerate(batch):\n",
    "                # x\n",
    "                img_path = img_paths[id_]\n",
    "                img = self.get_img(img_path)\n",
    "                img = self.augmenting(img)\n",
    "                img = self.resize_and_normalize(img)\n",
    "                  \n",
    "                # y \n",
    "                label = labels[id_]\n",
    "             \n",
    "                # Store the values    \n",
    "                x[ix] = img\n",
    "                y[ix] = label\n",
    "\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1223
    },
    "colab_type": "code",
    "id": "bJ-aVohsqt1F",
    "outputId": "5514493d-cfc0-47bc-c4db-7b5d3de32331",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.17849061 0.17849061 0.17849061]\n",
      "   [0.29878467 0.29878467 0.29878467]\n",
      "   [0.18046218 0.18046218 0.18046218]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.19532235 0.19532235 0.19532235]\n",
      "   [0.54050744 0.54050744 0.54050744]\n",
      "   [0.0875     0.0875     0.0875    ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.28125048 0.28125048 0.28125048]\n",
      "   [0.50807995 0.50807995 0.50807995]\n",
      "   [0.07924826 0.07924826 0.07924826]]]]\n",
      "(1, 224, 224, 3)\n",
      "----------\n",
      "[[1. 0.]]\n",
      "(1, 2)\n",
      "----------\n",
      "(224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOy9aZBc53UleF7u+77UvherUIUdIImFFAmK1AJRQ1GWFC2He2SLthyhUNiO8A/3+Ic9MR12ODq6p3+2rfbYHoviiJJlSTQliqYomiQIkgCIrbBUFWpfMiuzct/3Nz+K5+IVSHe7BYAkiLwRCBKJrMrMl++737nnnns+RVVVtKMd7bh7Q/dhv4F2tKMdH260k0A72nGXRzsJtKMdd3m0k0A72nGXRzsJtKMdd3m0k0A72nGXx21LAoqifEZRlBlFUeYURfkPt+t12tGOdtxcKLdDJ6Aoih7ALIDHAKwBOA3gq6qqXrnlL9aOdrTjpuJ2IYH7AMypqrqgqmoNwPcAPHGbXqsd7WjHTYThNv3ebgCrmr+vAbj/X3uyoiht2WI72nH7I6GqavDGB29XElDe57FtC11RlG8A+MZtev12tKMd743l93vwdiWBNQC9mr/3AIhon6Cq6rcBfBtoI4F2tOPDjNvFCZwGMKooyqCiKCYA/w7Ac7fptdrRjnbcRNwWJKCqakNRlG8BeBGAHsDfqKp6+Xa8Vjva0Y6bi9vSIvxffhPtcqAd7fgg4h1VVQ/e+GBbMdiOdtzl0U4C7WjHXR7tJNCOdtzl0U4C7WjHXR7tJNCOdtzl0U4C7WjHXR7tJNCOdtzl0U4C7WjHXR7tJNCOdtzl0U4C7WjHXR7tJNCOdtzl0U4C7WjHXR7tJNCOdtzl0U4C7WjHXR7tJNCOdtzl8SsnAUVRehVFeUVRlKuKolxWFOX33338/1QUZV1RlPPv/jl+695uO9rRjlsdN+Ms1ADwh6qqnlUUxQngHUVRXnr33/6rqqr/+ebfXjva0Y7bHb9yElBVNQog+u7/5xVFuYotq/F2tKMdd1DcEk5AUZQBAPsAvP3uQ99SFOWioih/oyiK91a8Rjva0Y7bEzedBBRFcQD4IYA/UFU1B+C/ARgGsBdbSOG//Cs/9w1FUc4oinLmZt9DO9rRjl89bspoVFEUI4DnAbyoqur//T7/PgDgeVVVd/5Pfk/baLQd7bj9cWuNRhVFUQD8PwCuahOAoiidmqc9CeDSr/oa7WhHO25/3Ex34CiAfw9gSlGU8+8+9scAvqooyl5sHTu2BOB3b+odtuOWhk6ng6Io0Om28v9WLr8eiqJAr9fL4/y7qqrveS4AWK1WWCwWGI1G+XedTodGo4FWqwVVVeW1qtUqisUiyuUyms0mms0mWq3W7fy47fg3RPvcgY9RcHHrdDoYjUaYzWbY7XY4HA5YrVZYrVYEAgHo9XoYDAZYLBaYTCY0m03UajU0m035Pc1mE5VKBUajESaTSX63wWCAoigwGAyo1WqoVqsAgFarhWazCaPRCKPRCKvVCgCo1WpQVRUOhwMWiwX1eh35fB6ZTAbJZBILCwvI5/Mol8toNBr4KNyPH+N433KgnQTu4FAUBWazGT6fDx0dHQgGg7BarVAUBUajETabDc1mE+VyGa1WC7VaDalUCuVyGcDWwi0WiwAAm80GRVFQrVbRarVgt9uh0+lQrVZlN69WqzAYDDCZTFBVFUajEU6nE41GA3q9Hnq9HtlsVha0oigwmUwwGAyoVqvQ6XRwOBzo6OhAq9WCyWSC3+9HtVpFPB5HLBZDLBZDOp2W9/xRuD8/RtFOAh+H0Ov1sFqt8Pv96OrqQkdHBywWCxqNBqrVKqrVKkqlEqrVKsrlskD1ZrMJl8uFcDiMzs5OBAIBOJ1OmM1mZDIZRCIRNJtNKIoCVVXh8Xhgs9mg0+ngdDoBAOl0WhZ2vV6HXq+H3W6HxWIRtGE0GlEul5HP52GxWJDJZBCNRmEymQAAm5ubsvPn83lUKhXo9XrodDq4XC5YrVY0m03kcjmsrKwgEonI8+v1ejsp3Fy0k8CdGnq9HjabDV6vF11dXQiHw7BYLFBVFblcDpVKRRaIw+FAIBDAwMAAfD4fgsEg/H4/bDYbHA4HvF4vTCYT9Ho9arUastksXC6XQH5tsF7nIlVV9T2LkFyB9me1z9NyC6qqotFoIJ/Po1qtIp/P48qVK1hdXUU0GsXKygoymQwsFgucTieMRqMkuEKhgHg8jmQy2S4ffvVoJ4E7KbjjdnZ2oq+vD4FAAGazGfV6HZlMBrlcDnq9Hh6PB3v27MGBAwcQCoW21f83LmydTielgMFgwMzMDP7hH/4BY2NjOHDggHAJ5XIZiUQC+XwerVYLVqsVHo8HiqIgm82iWCwKp6DX61Gv11GpVODxeGA0GpHNZlGv1xEOhwX663Q6QRbAVmJhWcGfj0ajuHr1KhYWFrC2tob19XX5nHa7HV6vV5JfsVjExsYG1tbWkMlk0Gg0PpTv6Q6LdhK4E8JgMCAYDMrCdzqdaDabKJVKKJVKUFUVgUAAvb292L17N3bu3IlAICCLg4u+VqthY2MDjUYDBoMBlUoFuVwO3//+91Gr1eD1elEqlTA1NYVMJgPgeuKpVCooFAqo1+uSGHQ6HWq1GiqVClqtFgwGgzzearXkMf5xu91wu92wWq3o6OhAf3//NkIyFAphYGAAqqrCZrMhFArBbDbLdSgWi0in01hYWMDFixcxPT2NVCqFYrGISqUCr9cLn88HnU6HQqGAaDSKeDyObDaLWq32oXx3d0C0k8BHNRRFgcPhQH9/P/r6+mTXLRaLKBaLKBQK8Pv9GBkZwb59+zAxMQG/3w+j0YhGoyEkXK1WQzqdRjwex/z8PObm5hCNRrGxsYF0Oo18Po9YLCbJgt/97boHmJBsNhvcbjcURYHH48H4+Di6urpgtVpRKBSgqirsdjvcbjfC4TAGBgbQ398Pq9Uq7cZ8Po/19XVMT0/j2rVriEQiSKVSqNfrMJvNCAQCAIC1tTWsrKwgmUyiUqncls91B0c7CXzUQqfTwev1or+/H8PDwwCAWCwmrTqbzYbh4WHcf//9mJiYQCgUgsFgQKlUQjQalQSxvr6Ozc1NbG5u4tq1a7IImCDeb2e8UQ/gcDhgs9kE/hcKBRQKBQCA0+lER0cHXC4X3G43jEYjFhcXEYlEBD2USiV5PluMAN6jBWCbkX8efvhhHD58GM1mE4uLi8hkMnA4HOjt7cXo6KiQl93d3fB6vdDr9Wg0Gkgmk7h8+TLOnz+PhYUFxGIx6HQ6QUWxWAwbGxuIxWLIZDJt7mAr2kngoxIGgwEdHR3o6elBOByGwWBAIpFAtVqF0+nEPffcg8OHD2N8fByBQAA6nQ6ZTAbz8/PY2NhApVKBz+dDs9nE9PQ0Tpw4gdnZWVQqFRHiaEOv18NoNAqH0NvbK9CciWBoaAh79+7F+Pg4Go2GQG7W5WNjY4JMAoEAIpEIYrEYgK2FTp5idHQUY2NjiMfjmJqawqVLl7CwsCAJTVEULC4uIhqNIpFIoNFoiODIbrcjEAgIUdnd3Y2Ojg4pZUKhEPbv34/BwUH4fD4YDAY0Gg0sLy/j0qVLmJubw+zsLFKpFJxOJ1wuF2q1GtbW1jA3Nyeo4y6OdhL4sEOv16OrqwuTk5Oy27LO7+rqwrFjx3D48GGBtvl8HpFIBGfPnsXa2hqMRqP09S0WC86cOYNTp06hXq8DgAiF9Ho9Ojo60NfXh/7+fgwMDKDRaGB9fR3FYhEulwv1eh3r6+sAtpJSLBbDwMAAnnjiCTz00EOo1+uYmprC8vIy8vk8stkslpeXkUqlEAwGMTAwAJfLhUqlIujF4/EgHA6jr68P4XAYJpNJILtOp0OxWEQ+n0epVMLm5ibW1tbw3HPPoVQqbbtORCk2mw07duzAwYMH0dHRAWCrxdhqteB2u9HX14eJiQkEg0HYbDZUq1WcPXtWrhe5DpPJBJPJhJmZGczOzr7n9e6iaCeBDyv0ej38fj927tyJ3t5ege587KGHHsLExIQw82tra7h06RJisRg2NzeRTqdhs9mQz+dx6dIlrK6uCsQ3m83o7OxEb28v+vr6hEF/4IEHMDAwgNXVVbz88ss4e/YsGo0GPB4PLBYLDAYDyuWysPOlUgmZTAaJRAIPPfQQvvzlLyOTyeD111/HiRMn0NnZCb/fj3K5DKvVCr1eD7PZLAy/qqpoNptIJBJYX1/H4OAgHnnkEQwMDKBYLOJ73/seTp8+LaKiarWKRx99FL29vTh58iSq1Sqy2SxyuZwkC+oWLBYLwuEwHA4HfD4fxsfHYTAYUCgUUK1W0dnZKVxJKBRCo9FAOp3Gm2++iVOnTiGZTKJWq6GjowOqqmJqagoLCwsoFot3GzJoJ4EPOvR6PcLhMIaHhzE8PIx6vY7V1VUoioJ7770Xn//854Uh39jYwOzsrLTHWB7UajXEYjGsrKygWCwK1Lfb7di/fz8+9alPYWRkBLFYTKC1oijw+/2Ym5vDiy++iGg0ilAohMHBQan5AaBcLksdz9ZdqVTC8vIyHA4HDh48iPPnz+PcuXMYHx/HyMiIqBGtVivsdjtUVZUypFaryR+Sm+Q9lpeXEY/HUa/XhZw8dOgQjh49Cr/fD7vdjkKhgFqthkQigbNnz+L8+fOIx+OoVCrCLVCybDKZ0NfXh8nJSSkhbDYburu7MTExgZ6eHrRaLayvr8tniMfjMJvNsFqtyGazuHz5MtbW1kT6fBdEOwl80LF//37s378fyWRS4PyDDz6I48ePo6enR+rZ+fl55HI5xGIxXLt2DWtra4hEIshms2g0Gmg2m1BVVSByKBTC448/ji9+8YtoNpv40Y9+hLNnzyIWiyGXy4kgiHV9OByW3ZsEXyaTQTablYVHgs9kMiGfz2NhYQHXrl1DvV7H6Ogo9u7di2aziWq1CpvNBpvNBovFgmaziXw+j1qtJuWIxWKRliI1Anq9Xq4LF53dbkc0GkU4HIbZbIbT6UQoFILRaBRkkEqlkMvlkEqlcOHCBUkgDL1ej1AohLGxMfT19cHhcMBgMGDnzp04fPgwPB4Pms0motEoTp06hTNnziCXy4lgKpVK4eLFi+/5vR/TaCeBDyqMRiN6e3tx4MABzMzMoNFo4OjRo3jyyScxOjqKUqmEhYUFFAoFXL58GadPnwYAJBIJzM/PbyP3WEqMjo5idHQU4+Pj2LFjB7xeL06ePIlnnnkGMzMzqNfrspBdLhd6enrQ19cHRVGQz+dhs9kEPZRKJSwuLkptrO3xq6qKcrmMlZUVXLhwAc1mE0eOHME999yDYrG4TcVXLpfl75lMBsViEVarFWazWZKWwbA1qMrn8zGiBL4eZcgk+0wmk3QG+BmuXLkitT5FT2x1MvkwATYaDYyPj+P48eN48MEHpUxYWlrC2bNncfnyZcTjcXi9XhiNRpw6dQrXrl37uGsM3jcJ3MwocTveJ8xms4h4zp07h9HRUXzlK1/Bvn37EI/H8bOf/QynT5+W4ZmTJ0/inXfeQb1e3zbFZ7VaMTk5iV27duHIkSPo6+uDz+eD2+1GPp/HmTNn8Nxzz2F2dlYm9YxGIwKBACYmJtDZ2YlarSZEIAeNSqWSEIR6vV7ad6zrCee5qGq1msiGOZBkNpsRCoWQyWSwubkpO32xWES1WoXf74fZbEar1ZIBJK1CkEKjer0u5Qulz81mE1arVZIDUUqr1cKOHTswOjqKaDSKy5cvQ6fTYWxsDM1mEzMzM8jlcohGo/KZ1tbWcPLkSdx777146qmncOjQIYyOjsLtdsNms2FhYQHT09MoFArYs2cPurq68Itf/OJuQATbop0EbmEEg0Hs2bMHHo8H77zzDr7xjW/g2LFjKJVK+NnPfoZXX30ViUQCNpsNhUIBc3NzSCQScuPb7XZ4PB4MDw9Le9Dv9yMSieDEiRMYGBhAIBDAxYsXhdjyer3bRD89PT0IBALI5XJoNpswGAyw2+2w2+0yKUjdPTX5TqcTbrdbSEIuaqfTKQRgoVCQx1utFiwWC3p6etBsNmWwiBwBBT43agU4Y8DXttvtALBt8tFoNArJqNPpZMyZSEWn0yEYDGJychKqqmL//v3YsWPHNk5lfX0diURCui8nTpxAq9VCqVTC2NgYBgYG8JnPfAabm5t4/fXXMT09jUgkglAohAMHDmBqaupu4gna5cCtCL1ej/7+fhw4cADlchmxWAy7d+/Gn/zJn2B6ehrPP/884vE4VFXFhQsXEIlEUCqVtkH+/v5++Hw+mM1m3H///fD7/Th16hQ2Nze39fS1O2SlUpEpP6vVKjCacNtiscBiscjwkNvtRq1Ww8rKClRVRTabRSaTgdfrhd/vl527UCggFouhUqnAYrFIZ4CeAXa7HT6fDx6PB7lcDuvr6ygUCmi1WjCbzbDZbJIEDAYD6vW6wGyiAiIFp9MJm80Gs9kMo9GIUqkkZYNW79BqtVCpVFAqlUTgtLGxgXw+L0KiWq2GRqOBUCiEXC6HSCSCQqGA5eVlJJNJfOITn8Dv/M7vwOFwYGhoCB6PB/V6HY1GA1NTU/jrv/5rdHZ2Ynl5GadOnZIW48cobk85oCjKEoA8gCaAhqqqBxVF8QF4FsAAttyFvqKqavpmX+ujGB6PB7t370YoFJJx3Mceewy/+Zu/iZdffhnf/va3Be5OT09LLUt1W39/P3K5HBqNBur1OoxGI6ampkQZaDKZRD5LWG6324Ulp4KuXC6jXC4LOvD5fFKvUw3IkWK9Xo9SqSQ6/mq1ikQiIXU6zUGq1ao8n/U7AEEQlDuPjo5ic3MT8XgcjUZDtAwk6VhqsJNA/sJoNKLVaqHRaGx7n6VSCUajEQCQzWbFjyCfz8NsNsPlckkiVRQF8Xgca2tr0v5cXl7G2NgY9u7di7W1NVQqFSSTSbz++usol8s4duwYVldXceDAAfT29sp7/PznP4+/+Zu/QVdXF770pS/h1Vdfxfz8/Mfe/eimkcC7SeCgqqoJzWP/CUBKVdW/UBTlPwDwqqr6R/+D33HHIQEKf/bs2YN6vY5IJILBwUE8/vjjePDBB2EymfDggw8iGo0KOUazjo6ODgwMDGD//v3w+/2iqqMslwuDtb7BYECr1doG1XnjamE9CcCenh7YbDbUajUYjUa4XC54PB5BDFevXkUikUCpVBKmnImG5iHZbBb5fB4Oh0NYd+7o5CzYlnO73dDpdIjH40gkEpLMXC4XHA6HmJkUCoVt4796vV5+RzAYlM/O0qFQKKDZbApiocaAcwQGgwEOh0OQAcePK5WKqBtrtRoikQii0agYlRw6dAif/vSnUSqVsH//ftx///0AtoaW3njjDfzd3/2dtHZfeeUVTE1NiSDrDo8PlBh8AsDD7/7//wvgXwD8q0ngTguHw4E9e/agp6cHa2trMBgM+NznPodPf/rTGBkZQaFQwN/+7d8iEolsg8elUgk7d+7EE088gUwmg5mZGSSTSUEFhPQk57gY2GZjvQxA6ndq/HO5HKrVqvgEmM1mSRbsq3N35eOsyfkaJpNJFjN32Vqthnq9DrvdjlKpJFoEk8mEarUqcmSShbQv42Lm+LDH4xFpcbVaFf8D/jx5BKICEpNUVAJApVKRXd1isUj5xN9DYpGuSOQB9Ho9hoaGkM/nsbq6iitXrqCnpwf33XcfXnvtNbz11lv4whe+gKGhIRw/fhzhcBjf+c53cOHCBdx3331wOp14++23P7YDSbciCagA/vnd3fyvVFX9NoDwuycUQVXVqKIooRt/SFGUbwD4xi14/Q8sFEVBOBzGwYMHUalU8NZbb+Hw4cP40pe+hF27dsHhcKBUKuHZZ5/FX/3VX8FisWB8fBzNZhPz8/NoNptYWVnBW2+9hVQqhbm5OQwMDKC7uxutVgvlcnmbZFa703NSkC0xjvRqCbdqtYpMJoNQKAS73Y5qtYpGoyGtNoPBALPZDI/Hg0QiIe3HYrEoxB7RA2t6h8MBs9ks5B7LAsJ5Mv7sHgAQ01E+n65Bfr8fGxsbKJVK8t7pfESxksVikZ3darVKx0FVVUESnBugqYrNZoPH4xELNQDSYjQYDKJl6OnpgU6nw/LyMl566SWYTCbs378f165dw9NPP41PfvKT+MQnPoH77rsPQ0NDeOaZZ/DLX/4So6Oj6Orqwssvv4x4PP7h3Hy3MW5FEjiqqmrk3YX+kqIo0/+WH3o3WXwbuDPKAZPJhJ07d2JsbAwbGxsol8v4gz/4Axw7dgzBYBCtVgurq6t4+umn8f3vfx+bm5vo7u6G3W6HXq9HMBgU3X06ncbi4qIM9mj758DWzAB5AjLilNByMXN31BqKclGwf8/Fw383m82ye5NI03ILXEQ2m01cgev1+rY2H1GFxWKR+f9yuSyP63S6bb6C9Amk6o9IoVqtimEId/FcLgeDwSClicFggM1mk89L5MJoNpswmUxCfPIzcFCICMHn8yEajSKbzWJoaAgmkwlzc3N45ZVXYLPZ8Mgjj8DlciGVSiEWi6GzsxPBYBC/+7u/i6GhIXznO99BOBzG8ePH8corr2Btbe09Q1p3ctx0ElBVNfLuf+OKovwIwH0AYoqidL6LAjoB3NHpMxAIYP/+/eLGs2/fPnz2s5/FoUOHYDAYkM1mceLECZw8eRKvvfaakGy7du2Cx+NBNBqF0WhEX18f7HY7ms0murq6tt20LpcLFotFpLy5XE7MQimWoRowHA7LfD75gkqlIrV6rVZDoVCA2WyWrgBhM7BVzoTDYRH4EElwAXs8HqiqikQiIUrGcDgsCIHlgMlkgtlsloTAnwe2yEMiArbbOKpMfQDfK4NJj4kNgMB7dkJYFmnLEK2NusFggN/vl64EeY177rkHFy5cwMrKCnbv3i3cyA9/+ENsbGzgM5/5DPr7+5HNZqV80el0+NznPoeuri785V/+JTKZDB544AHMzMzgwoULHxee4OaSgKIodgA6detAUjuATwH4vwA8B+BrAP7i3f/+5Gbf6IcRRqMRO3bswNDQEGZmZlAul9HR0SFQWqfTIRKJ4Mc//jGmp6extrYmYhWLxYJWq4XZ2Vlpw3V0dEjP2+FwoNlsSh1Lg1CTyYRQKITu7m5ZuI1GA263GxsbG+KsEwgEZMERIbAlWK/XUa/XYTKZ4HA4hDwjbOfuWalUtpUadA0mj8GFRFcjt9sNk8kkOy3VgWznsVSg2IgLVHsOAUeXWVLQWJQIQMtfsM2pbTXS8ZjDSwC2vW+iFYqcgOvS4qGhIZw7dw46nQ779++H0+nEmTNn8Prrr6PRaGBsbAyHDh3C6uoqZmdnsWvXLtx7773Yt28f/uiP/gj/+I//iHfeeQd79+6F1+vFa6+99rHQE9wsEggD+NG7NZwBwDOqqv5cUZTTAL6vKMpTAFYAfPkmX+cDD5PJhH379mF0dBSnTp3C6OgofuM3fgMejwd2ux1dXV04e/YsXn75Zeh0OnR1deHtt99GoVCA1WrFzp07sbi4KCVAMBgU2E0EwNq/Xq8jGo2iXq9LDa7V2tM9p7e3V4aIePOxDcia3W63o16vS8uN03daVp6kIMk5JgNVVaUdxt2Y7c1MJrNt0THRAJBuBrUPrPO547MVyMVMjoEzEVolIROfVldgsVjg9/u3mZ3yNVkm8Wd1Op24LQOQx5iYBwcHkUwmce7cOfT39yMcDmNpaQlnzpwRUpaCr6effhoAcOTIEQwPD+O3f/u3MTw8jBMnTqCnpwePPPIIXnnllTueMGyLhTShKApCoRAOHz4sI7xf+cpX8PjjjwsZdfXqVTz77LMYHx9HT08Pzp07h6mpKZw8eRKFQgHDw8PYvXs3lpaWZJfXeui1Wi0Eg0F4vV6Bq5wWLBaL0gI0Go0IhUIy8UbUwLYhF4LJZILb7RZGnOy60+mUOXt2GoCtRcEFy947x3fZQgO2hnxo+R0IBLB3714ZxrFYLPB4PELc0f4rm80CgCALJhFFUdDX1ycEKGccWDJoFzMNQ1kOcFxYm3ioKKzX63C73UImNhoNxGIxRCIRmUUgN0LXIpKvHILKZrNiqlqpVHDgwAHs2bMHpVIJgUAAx44dk9ebmJiAw+FAJBLBxYsX8dJLL6HVaqG7uxs//OEPEY1GP4S79n8p2rMD/6OgHPXgwYNYXFxEPp/Ht771LXzhC1+AyWRCuVzGz3/+c1y4cAFf/OIX0Wq18E//9E/Y3NyUMV8y4qlUCn6/X2penu5Dko4LjQiBra9isSgw3m63S91LAU08HoeiKHA6nZI4qCIEAK/XK4uThBwTRaVSkV21UCjIAtHKcVlXA9eNOJhMTCaTvB77/mxp5nI5WXQA5HPydKNGo4FoNCoz/dQVABDtA41SmQT5OhRCsZzQnoKkLYNoVJJOp+X7rFar2NjYQCqVQqvVEis3kqssGyi0WllZwdTUFOx2O3bs2IFkMokXXngBn/rUp+D1ejEzM4OBgQFxhPJ4PPjJT36C+fl5HDlyBCdPnsTGxsYdN3vQTgLYQgADAwO4//77cenSJYRCIfzpn/4pJiYmYDKZEIlE8MYbb6BQKOCrX/0qZmdn8frrr6PVaiEajWJhYUF2SIfDAY/HI4SdTqcTbTwAsdcGIBNzwPUz/bg7OhwOABDFHo1AeJ4ff16n06FSqUgtzTodwDYBEVuPRBvkLMxmM8xms6AF7sjULvCxer0Ol8sli7rRaCCbzYoCkLs+ic6NjQ34fD5YLBa5xmtra8LY83Hg+tkF7IKQT9AiBCZi8iOcgVBVVToN9CuwWCyYnp7GuXPn4PF40NPTI0mVMmrqLEgy8mSkxcVFvPPOOzAajRgeHobBYMALL7yA3bt349ChQ1hYWMDc3Bz279+Pw4cPQ1EUPPvss6jVanjsscfw+uuvY2lp6Y5KBHd9EtDr9RgYGEBvby9OnDiBiYkJ/OEf/iEmJydRqVTw05/+FGfPnsWDDz6IXbt24bXXXsOlS5dgNpuRTCbF289gMGBoaAj9/f2ycEh68WbmDU4xTKlUEntvcgTcoQm5SQw6nU74fD6BtM1mU/gBtvxIAPIxLSLgQiuVSlInk4zTdhm0g0Ld3d0wmUzib8Dn8/Nwp50Y0OUAACAASURBVKaqD9ha7KlUattEotvtRjAYBADE43HplHD3J5HIFinhtxZRkFdgEgEgj5XLZeRyOWljLi4uYnV1FX19fQiFQnKyEbkBIiBKnNlxqNfr2LVrF6ampnD+/HkMDg7C6/Uin8/j4sWLaDabOHjwICKRCN58803cd999OHToEPR6PZ5//nlUKhUcPXpUrNzuFLnxXc0JGI1G3HPPPQgGg4hEIpicnMRv/dZvYc+ePSiXy/jJT36CRCKBo0ePIpvN4tVXX0U2m4XP58Pq6irefPNNuflDoRD27NkDl8sliyoUCgnxx12MpByFLIVCAZlMRnZ8YKuV5nK5RObL/juHXQBIcrBYLHKD83Hu+trFT2lxMpmURaQl1FRVleEdPpeohBOPqVQKPp8PPp9vG/SnUIgS4Xq9DqfTiUKhIC03p9MpRh7BYBDj4+NwuVxSixOeA9cdibUlDRMokU+1WhX0sLGxgfX1dZRKJSSTSUSjUfh8Phmn1hqb8CATKhmZhHmEW7PZxJkzZ7C8vIz+/n48+uijkljpyvTZz34WoVAI8Xgcw8PDcLvdeOutt/C9731PSqa33noL0Wj0o4YI2pyANvR6PUZHR9Hb24u5uTk88sgj+OY3v4lgMIhsNos33ngDHo8HjzzyCF599VW8+uqrsoPEYjFcunRJEoBer99GznG350m8iqLIQA9vCjLivNG5u/PnuROyb87WGhMBh3+4U7KG5sLXqg8BSM28sbEhzDrRAiExrbeArQRJ4ZHdbkcoFIKqqtId0dqG22w2+TwU0XBUmLwIW6oGg0ESJb0JtIucj2s1AwC28SoA5LW1nYlKpQKr1YrBwUHxJCAPwjMc6cHI75JnIeZyOSEth4eHkUgksLS0hKtXr+L+++/HxsaGXMdnn30Whw8fRldXFy5evIidO3fi4MGD0Ol0+MEPfgCdTif3zerq6gd3U/+KcVcmAb1ej8HBQXR3dyOXy+HYsWP42te+hlAohIWFBZw+fRqTk5NoNBr47ne/i+npaSGzCHdTqZQo2ZgEKFFlKyuRSEiNzmSgJeLY23Y6nSKPJfHHPj0TB6E3AOn/A5D6nMiA/IH20FDWzDyuizU8h204J8D3xv/nf9nqGx4ehsPhkATD30FYThafycdkMgn8phrR7XaL23GlUhGob7PZ5EgzfmYmM22pROKUXECtVhOzlLW1NUFN/J49Ho9wNBQ38feTE+GINX8fRVb5fB5Xr15FZ2cnfD6fSJTL5TJeeOEFHDp0CPv27cPFixcxMjKCvXv3Ip1O45lnnpEWc6VSQSKR+Kghgm1xVyaB7u5ujI2N4erVq9ixYwe+/vWvo6+vDzMzM3jnnXcwOTmJ2dlZnDhxAqVSSQQ4TqcTmUwGly5dQr1ex8jIiJhX5PN5FItF6b9zB9YuDlVVZdINgNzYlOFyJ9UKZ5gsaOZJoRHFPoT7rJW5SzNxEObyyHAOAkWjUdhsNoTDYdllASCXy8Fut0sSoWa/UChIGcDX4598Pr+tBNF2GbSDQTRN4Yix1nyEjD3LIpPJJFOA2uEpKg61LU+ajrJrwp2fwiiSlpRMm81mlMtl+Xd+D0Rh7FgoioJkMolTp07hM5/5DBwOB1qtFlwuF+x2Oy5evIhyuYzx8XFcvXoVJpMJDz/8MOr1On70ox/BZDLhgQcewJtvvilI4qMYd10SCIfD2LVrlxh/PPXUU+jr6xPya9++fXj77bdx+vTpbTcJ5/tPnz6NTCYDt9uNzc1Nqf2XlpZgs9kwMjLyHtsutrNI+pH55g1L2y7u5Nx9KdHVuu3w3wmhuXgJewGITl87eptKpbZ5+kUiEYHqqqqKpTkhd7ValUNCC4WCHBtGpEL0wERQq9VkV9fyE1y4HCJiyaRtlVIdqO0KUCbMmQhqLphcqRxkm499f4qbtIhMW2IwoRJxMEGUSiXxLjAajfB4PCgWi8hms+L2PDg4uG2QymazYW5uDuVyGQcPHpRzHB5//HGEQiH8+Z//OcLhMB544AH84he/+MialOg+7DfwQYbP58OOHTuwvr4Oh8OBb37zmzh48CBisRiWlpZgMpnw85//HCdPnpT6mzeqoiiYmpqSjE6obLVaUS6XYTKZEI1Gce7cOeRyOWnTaZVwhPC5XE6m9bjgAMhuyBuear5MJiMtuhvbY1o4zZ9lwqnX6zKLz3qc5h8WiwXd3d0iXHK73fB4PFLf8yj0gYEBhMNhABCxTr1eh8fjEaGOlujkDqpNBvxdTqdTUIaW++D1ZKnCxMTvQNs90B6XzuvLbofWa1GLBLRmLOROeI1ZGvEA1VarJc5Dvb29sFgsqFQquHjxopCcTErUUUQiEVy5cgWNRgOpVAqlUgm7d+/Gk08+iWQyKcNn5Fs+anHXIAGbzYbdu3dL6+5b3/oWJiYmEIvFxHn2ypUrSKfTsNvtQpxxMV66dGmbywyhPQ/6pFV3IpGA1+tFV1eXLHRgi9iKx+PSWrNaraLoazQaAvu5I2qn8bizmc1mVCoV5PN5mRMgT6A17+RCaDab4rPHY83z+fw2h2B6CRJF8Ge5gHncOJ2EOPXYarXEwZdyYT5OhABsLVav1wuv1yuaBC4k7cLmdaWuQvu53o8wpFSaOz93dgDSIuUIMY9Y1/IFbBfy+7XZbOjo6JATjkwmE3p7e1EqlTA/P49IJIL5+XkcPHhQ0EClUpESbG5uDtVqFQ888ACWlpYwPj6Or3zlK/D5fPj2t7+NwcFBHDp0CKdOnRKdx0cl7ookYDQasXv3bqkLf+/3fg9HjhxBsVjE6uoqlpaWcOHCBWGKSWbxz9WrV3H58mXRyrvdbvHc6+npkcWYTqdFtba5uSmEVbVaFZKMN26pVEI+nxfVIHAdNZBEpMAI2G7pRX8AbZICsE2LQKUhe+QkHf1+v6jwtCQlFyFVftyZiVLoPMyOQrPZRC6XE5NS+gWSh9CWKkRBNC7lLg9ACEht0uB3RjKUTD5LHnIHWs8CrUMxAOE92MHRBj8znY94rSkn5gyB0+mUJFiv17G2tobe3l74fD75PolU6BlhMpnEMm7//v147LHHsL6+jueeew67du3C5OQkTp06dWtv8JuMj30SoKuMx+PBysoKfu3Xfg333XcfarWaSH4vXLggjDcA2QnppDs7OysJgHZbwWBQevS8QUulksykr6ysyMk4rO9vJLf4eKvVktFi7bgsa2qXyyXEIute4Pp5AYpy3QGIZQqf7/F40NHRgeXlZSiKgkAggGKxKKUHX5O7sV6vh8PhkPehVdYxeTgcDllwhMTaCUaeT8D5AE4lUvNAopDSXi0y4MJmqUPtAF+bz+Xn5x+TySRtPpPJhFgshng8LkmCuzcAQTTNZhOVSkWGtrRqRg5LUdjFUouHqzocDuEtOA1JTwlVVZFMJlEoFPCpT30Kv/7rv45SqYQzZ86gr68Pm5ubHylV4cc+CXi9XoyNjcFms+HRRx/F5z73OZjNZuTzeczPz+P8+fPb1H282XmDXr58WQgdg2HrNOFgMIhAICA3LJOAx+OBz+cT45ClpSVh6wn1geuQl1CWCUjb+yZDDUBsw1j3a3dZ3thadZp2gAeA9PhTqZQw+VyQTDR8X6zfK5WKmI5w9+ZC4uehEUi5XJafoxMx63IilVqthnw+D6fTKbW+FuozOWoVjtzttdAfwLbkyce0g1jxeBzZbFZmJuLxONLpNILBIEZGRpBOp6WzQqTGxEeUQrjPzk0ul0Mul8Pi4qKch8juBr9bi8WCfD6PaDQq4igKrJ566ilUq1Wsrq7i8OHDUjp+FOJjnQQMBgNGR0elNdTT04NMJoN4PI5kMompqSkkk0kpAbS1N70CyPgqytbx3Z2dndusv7Q1LiG03W6Hw+FALBaTXZELh3Urn28wGKReZhLgAtDWwgDkv1xghOtcDBxB5q7KRWAymTA4OIhQKITp6Wlpy2l3WyYGCnkAyOtTocdFWyqVhEDkEWRer1dkx16vV1SMdP/RkqAk5NgK5HsFrh9RZjAY3nPMOhMkSxIuYNqvcyqSj9N/oFarIZPJSLlHUo+DWYT/DodDLNZ4bYhy2GKtVCpYXFxEX1+ffKcslbQ2aLFYTMq6/fv3w+12Y+/evZiamoLX68X4+DhOnTr1kTjx6GOdBAKBgOyC0WgUf/EXf4Hx8XHs3bsX2WwWa2trAnUJwQmPK5UKNjY25CY0mUwIh8NSj1NowkXDhcVdq1KpoLe3VxIEzwQAIK0/m82GQCAgXgM3OtVoW2dcmJx60+6iXEg3PpcLhgnDbrdj586dSCQSAv3JV2gnFrWcRSaTEXREhWI2m92WCICtpMaxYi5Sip4AyIIh+tBq/VlyaMlB4Do/QNck1uHU+3MnTqfTUl7xfWtLm0KhIJOG09PTgkh6enpE1cmypLu7GwBEym2z2eD3+5FMJiVxxmIxxGIx9PX1bYP0FCcVCgUZlqrVarBYLNi3bx8efPBBxGIx/PM//7Ocn3j58uUPfcbgV04CiqKMYetsAcYQgD8B4AHwOwA23338j1VV/dmv/A5/xTAajRgbG4Ner0dPTw+OHDmCf/mXf8H6+ro45RKSAtdZcS7ozc3NbSxutVrF/Pw8JiYm4HK5YDAY4PF4JAkQQmqlvdzRq9WqoANOC3JcmLsFbwT2wrWlBksDtru0sBm4TnQB1xcOF4X286mqCofDIeQfiUruRjqdDoVCYduCYt1LPYPZbMb09DSWl5cxNDSEvr4+GI1GcfWluvBGjQATlNZhiIiFZJ72c/Dv2knCcrksuzw5CSotgeu8AQlM8g8dHR3o6+uTcey1tTUUCgWsrKzA5/Oht7dXFIzd3d1wu90ol8uoVqswm83o6+tDPp9HPB6XKc5IJILR0dFtSI2dGXo5ZrNZdHZ2IhqNwu12Y2RkBMePH8fc3ByuXr2K0dFR5PN5LC0t3ZY18G+NXzkJqKo6A2AvACiKogewDuBHAH4LwH9VVfU/35J3+CtGOByG1+tFJpPBU089BafTiRdeeEF2LrbNCE2pMuOC5mAJg/CRvXQ61XAQhTcdcH2oBoAkBa0ppnaGgLsh2XAuHiYB3uDapGC322V6jouLv0db52sZd44Oa23CtTU1hTHJZBKlUkn651rVIo1PqL0notHqHoiqeF34fnkdCLMtFgs2Nzely8Fyg4mEvAzhdSaTETETrwV1BUQfTIDadqL2oFOWPUQUiUQCm5ubyOfz6O/vh91uR7lcFr4EgCS1/v5+SQyt1taR55lMRroJJBpJTBoMBuGFBgYGMDMzA6fTic7OTnzta1/Dn/3Zn6FQKOCxxx7DT37ykw/VxfhWlQOfBDCvquqytq31YYXFYpHTf/v6+jA+Po7XXnsNqqoiEAgIBObNQQTARQYAqVRKSB8A6OnpwdjYmBzCwb47xSB0wtXagpFUYw9eW/ey7GCdTH5Ay8Zzx9dCevbJtSQiISchtdbSi4mONz/Jwmw2K337ZrMpw0WsjbX2XdzpeK2sVisGBgakTZrP5xEKhYQQ9Hg8Uk/zczIRkm2nAIoyaaIEkpGKoiCdTovrDw9LYTJjeUK0wf/SU4E7MR8j2artpPA1V1ZWZKQ8m83Kd0Y1IREUZ01oqhKLxURFyA2Degleh3K5LAfA+nw+BAIBDA0N4Y//+I9lSvXQoUN46aWXBBF90HGrksC/A/D/af7+LUVR/ncAZwD8ofoBH0HW29srY62PPPKIEHO7du1CJBKR+ly78wPYNhtPYQ8Ases2mUyyo1G0YzQaZST2Rs08FzB/1ul0wul0CgzngmAtzRqdOzwXuMvlEoMOrSKPC5qLgmYl3FX5ftm+oiy3VCqJuzFfg7Jik8mETCaDUqkkJQxhNVuMx44dg8vlkpN5KCGmAIqThkRMTLq81iyRWK5oJwT5vvV6PeLxuIwIk1BkuULnI6IwXgv27VlyaAVJJBpDoZDYxPO7J4qix4BWD8FBL44nr66uotFoYH5+Hh0dHaIE5FkQwPbShqhyenoaHR0dGBwcRDAYhE6nw+bmJnp6ejAxMYELFy6ISOqDjFtxFqEJwP8G4P9496H/BuA/YutQkv8I4L8A+Pr7/NxtOXzEbDZjYGBATq4Jh8MCA+maC2BbH1x7c+p0OqRSqW3uP5Sb8gvijkzoSgfhG0UwhOTsoRM+crKQ9TYAMcZgQmBQ+5/P54WYZOuNC5MGIxzd5cCP1WoVtEMeIJ/Pb1MnGo1GdHZ2wuVyyW5L7z26JxsMBvT19SGTyWB+fl7+Pjs7K5Lljo4OQUoc141Go6hWq7BardJ5sdls4l3Y09Mj/MONXAY/F92NSQ4yuWp9AvL5PBqNBrxeLxRFEZRB5JLNZrd5ORAp8fppJcf8fonMeP04w9DV1SWt1pWVFfT09GByclK4HaIO3i8kB8lpvPHGG3A6nfD7/Th+/DgURcHCwgLGxsaQTqexsLBwq5fE/zRuBRL4LICzqqrGAID/BQBFUf47gOff74fU23T4iNfrhcvlgk6nwwMPPICBgQFEo1GcPXsWiURCalaOj7IlqG2BJRKJbWO7rGm16j9tTc0bVmuESRGRVhFHiHqj1p9wVqv+4++iPz/1/1qxDBGOoiiy82q7G5ubmzCZTOju7kZnZ6ckLf5spVIRmbTWnTgUCqFarWJ4eBitVgvxeFxq2sceewzxeByLi4sYGRkR4owtQb4HmpJoOxA+nw+vvPIKvvvd78Ln8+H48ePioEQExKlEDjDx+tAejElAm7zj8ThmZ2fh8XgwOjoqZz3cOFuh5Qy4u3PxU/XJQSTu6NqzF5kwSqUSrly5gnK5jHPnzol+hMlWy31kMhlBc7lcDidPnoTP58Ojjz6KkZER7NixA7FYDIqiYM+ePUilUh/4oNGtSAJfhaYUUN49dOTdvz4J4NIteI1/U1itVoyMjEgNPDk5Cb1ej9dffx1zc3PC/LNXD1xXB1LAE4/HEYlEAGztwmzhFYtFxGIxSQrsQbNuZylBZABsHZAZCASkX07oqpXDcsciNGd7S9v6004MUoLL5/DnWPtydwe2yoJoNIrTp09DURQcPnwYExMTglIo8lFVFZlMBlarFd3d3WJ8ysRC09RCoYBEIoFMJoPOzk4MDg7CaDRKiUMBDmtubQnQarVw8uRJPP300yiXy9i/f7/stjzpiEmYn9NsNiORSEh5pO2SaNuhTHA0fN2zZw+CwSASiYT0/bm7a5MHB7u0ycHhcMBoNEpniG1QtiaNRiO6u7sRj8cRjUaRSqVw9uxZ3HvvvdKB0HYL+PPRaBQjIyPYuXMnZmdn4Xa7cfToUezcuROZTAYLCwtotVoYHh7GxYsX39Muvp1xs4eP2AA8BuB3NQ//J0VR9mKrHFi64d9ua7hcLoyOjortU1dXF+LxOC5cuIBCoSCZmmw0SSkaSfDLIiPvcDgQCoVECMKTbeidpx3aIcPPlhsXFttKPASEC5hkJGEuD/RgO047Bw9cNyKlBJfDOmTotW1DvV4vcmUKZZaWlvDMM8/g93//9zE4OChEGdn+QCAgE4vcKUnoNRoNBINBWK1WRCIRdHR0oKOjQ64fe+gkIw2G68eHMaGcOXMGJ06cwK5duzA+Pi4wXWv2ydFnQme6E/OzkA9gomESsNvt6O3thc1mQyKRwOXLlyVxAZCkQQTHep8kMHCdw0mn00LOaoVLRDnJZBLFYhHDw8OCpDY3NzE1NQW/3w+73Y58Pi/JhoYyqVQKq6urCAQCSCQS+MEPfiAKRh5sa7Va0dfXh0QigeXl5Q9q2dxcElBVtQTAf8Nj//6m3tFNxODgoMyWHzt2DB6PB7Ozs0gmk6IDYKmglXuyvubcPeG2z+eDy+XClStX0NXVhaGhoW16fq2lFskpqteoQacf/+bmpozccnKPO3+lUoHZbEY4HJZ5eyIUJga/349QKAS9Xi9z7zeO1lLRxjMECKv7+vowODiI1dVVnDlzBpubm9ixY4fMJLAs0ioh+f7Z9tLr9SKXJnLijsprwKQIbDkdl8tlZLNZrK+vI5/PY8+ePXJyEr+nVColZQ2TJ1HOjTCepiFMVvl8Xroy9XpdpjKB7cegke1nacXvkJ+dyIWIRNtmZQuwWCyKRVsul8Pg4CDuvfdeLC8vy9Hnb7/9Ng4dOiSdIwAyXzE0NIRUKoVf/vKXOHToEGw2G65cuYLu7m4Ui0WZPPT5fJJYyF/d7vhYKQZ7e3uh1+sxMjKCQCCAcrmMt99+G/l8Hl6vV24+stDcZdnXXVlZQSqVArAFA3mIaKu15VnPkVqy9NrWG+EwANEccNFwYZKBrtfrwk+kUinU63Xcc88924wx+R45fcc5fO2/caKOuylPRY7H4wJd3W637GwcUlpbW0M6nUZvb68ccAJA9AvcIbXHiRFBURqsfR7ray7YSqUiZhzLy8vSwuTcA1uiRqMRmUxGBqg4u699L3wtJkV+Vh6qQjMTKhZZ32s7P/x+tV4C1DxoTw/i/VAqlSSRAhAuJx6PI5fLyWN79uzBww8/jDfeeENGzU0mEw4fPixtSr5WvV5HOByGqqqYnZ3FkSNHEI/HcenSJdx7771YWFjA5cuXEQqFMDo6inQ6jYsXL34gQ0YfmyTA+i6fz2NwcBAdHR24evUqpqentxFKAATqAtdnzyuVCpaXl2Vct6OjA2azGTMzM9vgtsfjkYXKhaiV7QLXLcVY52rbZWxlsYdcLpcF5jNRcOHd2JqkNblWfENdALsNfA51Azcaimrt0lZXV5FOp+H3+xEMBlEqlbadf0CDUDLfhUIBRqMRgUBABoJ4DVnWcLouGo0iEokIyuGi1xJnTHIkA1l2aclUh8MhpKHBsHXYaDabFQ8HXkOWFFo+gew8rwM5Ie0EJxNZNptFKpUSrQQRicGwZYwaCATQ39+PUqmE9fV1zMzM4PTp07j33nu3cRnXrl2Dx+MRj0oAUhIBW2hmc3MT58+fh8vlEn/Cp556Ck8//TROnjyJUCiEnp4eXLt27QNBAx+bJEDfuqNHj+LQoUNQFAWxWEwYdgDbrLkI9whtU6kU4vE4VFWF3+9HX1+fwNharYbZ2Vk0m9dPE9YuTt7QNLTkzZdKpaSuZL9Zq+6j1pwKNZJxvPl4/h6hpVYmzJubv7vR2DoMhAIZ7dgug1Jd2poZDAZEo1GBpTxlmUM3hPhsyWndetgB4euTC1lfX8fKygoSiQRMJpMcgc7X485us9mE1GQJwePWtLoG7awGCUa2UXU6nZxYHI/HkclkYDAY0NPTI98PTU/I27Bk45kNpVIJsVgMqVRqm59hIBCA3++Xmp7yZvoFGI1GrK6uIpvNYmJiAslkEqurq6hWqzh37hyMxi07ewrLtCcyu91uGWHO5XJ47bXX8MQTT+DLX/4y0uk0NjY2EAqF0NXVhbm5udu+dj4WSUBRtswytL3jhYUFvPnmmwC2si+fp/Xk42IrlUpYWlpCoVCAzWbDxMSEEGqDg4NSk/KobtpxsYZmEigWi++B79pZ9FwuJ7sah1vYatTuVBSR8HNRgstd3eVyQVWvHyRKCE6nII/HI9CXuxr71mS/OYFoNpvh9/sRiUTws5/9DAcOHMDIyIigGirgqHYEIOo4tv+ALUHV6uoq5ubm0Gw2pa/OxXZjK44DTcD1o8hYAmSzWeEM2J6z2WyioygWi5IUKHTiPIDb7ZYRXXY0yHdks1mxANdyAZRS22w2KUuYYJhwia4MBgPi8TjK5TIGBgbQarWwtrYGn88nkutisYjz58/DarXC79+izLRJjN/z5uYmrFaroKbe3l587Wtfw/PPP4/Lly+jt7cX0Wj0tjsRfSySgNlsRldXF0KhEHbv3g2TyYSVlRXE43HZVbQ7IndvLtC1tTWsrKzAaDTKYRIzMzPo7+8XnkGv12+bUMvn84IyWIOy9k+lUjJkpB0U4iGkuVxO1GUkqLjLsjxhcnK73dJW5A6ubR+SuGNfmgmGDkK86Tgc5Pf7ZTHyc9jtdrhcLszOzuLv//7vMTk5iQcffBBdXV0AsG3ohwpBLhIy94uLi7h27Zp0QrRyZt702klEEnb0JqSBC4eWarWamISwNCPSIjpjgtLpdBgeHhbGn9eK11mv12N9fR2xWEy0EJz3J0/EyUS32y0lCYVm2WxWzm9kWUhuyG63I5VKwWw2Y2hoCLOzs6jVakin0zh79iyOHDkCv9+/7Rh4rTVcs9lEPB7H6dOnEQgEpFxUFAWDg4NYWlrC4uLibV0/H4skwEGeoaEhhMNhZLNZ6btqB1duXDxkfpeWllCtVrF7927s3btXrMRCoZDsToqyZeultaPSataBLYKRJ/eSpOICZ3vQarVKB6BSqcDlcslhpmT8CUHJrtOIg/Wrlskn1HU4HHC5XKjVavKY1WoVXwPqCAhPiRQowuFNt7q6ih//+MeYm5vDww8/jJ07d8o4rvacAL1eL7qKV155Bel0GqFQSMoaoifyINzVqcdwuVzS6mSCZfnk8XjQarVQKpVkQbE0yufzYu9FcjUQCGxj+skBUWzE3zs6OopgMCgbAPkBoiuixEAggI6ODuj1evGE4OJn52dkZETuh1AoJN95vV7HtWvXZEDp2rVrwm2QWGXCZumYSqUwPT2NAwcOiFhreXkZqqpibGysnQT+LUGfPyrT5ufnsbS0JBBYKw3VLia9Xo+FhQXEYjHodDqZOlxZWRF9uU6nE7hIiMsMDkBqzI2NDczMzMBgMGBgYEDab1wwvEEAyEguFyJ3S4paisWiKNRyuRySyaR43TEpbW5uymgta1273S5dEB4WSmKtUqnIISdUJhLh8FwEj8eDvr4+XL16FefOnUM8Hsfq6iqOHDkiyU/Lyr/11lv46U9/io2NDRw7dkyGq4gEAAhBSgKV8mGWQuQJuOtzypI7JgAR4LDUYnlEslCrtiSUZ8Lijs+yKpvNbnNVIqlHvURPT48kMBKb1C1ks1khP9lqJBHJlCMedwAAIABJREFUjWFsbEyQUaPRwMzMDOr1OiYnJ8VtmoNTvI/Ydbpw4QI+/elP4/7770cqlcL8/Dx8Pt9tXz93fBLQ6XTC3nLMd21tDclkUlpmWlaeX5bD4UA6ncaVK1dELKTT6YRM7OjokKET1qSsERVFEcMNinVCoRAKhQLm5+cRj8fR1dWFYDAojDWHb4rFItLptBB+JKO4yDY2NlCv19Hd3Q2Px7NtMMjv98NoNCKfz2Nzc3NbCRCNRuHxeBAKheB2uxGJRFAqlWA0GtHR0SFzE9wBtfoGRVGkpdjZ2SlDMpFIBC+++CJSqRQ++9nPYnh4GIqiyDFtP/jBD5BIJPDQQw/B7XaLKQmvs1ZGq1U8cmHycBEiLYqAWHpo1ZW8hjSJ2dzchMPhQH9/P5rNJqLRKDY2NqTsWltbw/r6Omw2G/r7+6UjQtRosVjg9/u3ScC1o9h0EqrVashms9jY2BCOhkmEPAG5Ju72AwMDwh/VajUsLy/DaDRiaGgIDodjm/aCqCWTyeDixYvYvXu3nIHYbDZx6dKlbZ2t2xF3fBKgt12tVsOBAwcEgjUaDWQyGTidTni9Xhmd5e4DAHNzc9jc3PI+IUlFln99fR1GoxHhcFjUhRQJkRSkNpzGFsFgEAMDAwJhk8nkttYe36PD4cDy8rLclNwNWZdyArJQKIjjLeE+jw3jsAyhPg8PsdlsGBwchN1ul742fQyoXiT/oC11PB6PXIe+vj5EIhEhH1999VVUq1U89thj6OzsxMWLF/Hyyy8jkUiI+Ic9dXr70WxFVVW43W5BAZQ9c1Hr9XpZLEwa2lYh+QKqK8nz8PtoNptCGHLYis9ttVoiwFlbWxNURjkvFz+TE7kdHkTCZMk2r8/n26bWJGeQy+UErej1W0fS9fb2ikEtkzQnN8lbsCRgco7H4/j+97+PgwcPYmBgAD6fT4axbmer8I5PAjSI9Hq9CAaDOH36tLTJ6BpDdpU3gNFoRCQSwdLSkmj6eRBpLLY1/1Qul8U+jC0q+tTV61uHb3R1dcHn80lrjjMDNpttm6FmuVyWmnptbQ1PPvkkxsbG8OKLL0oty+O/vF7vNhES4TEA6SyQgOQBm4Sv1WpVrLaDwSB6e3vlKPN8Pi9lA3D9QNR8Pi+JgkehU13HbkOj0cAbb7yBQqGAT37yk1haWkI8Hhdegu1M4Lr/IduZRE1msxk9PT1ykAlZe0Jh2piRRNW2cUk2WiwWKat8Ph8SiYQcrsKJTJPJhKWlJSHrOL7tdrulFNLKmdm2AyClHK85H7dYLKLW5DyJy+WShKE1QOEQEucLuMmUy2Xxnujr6wOAbZwMpzvPnDmDCxcu4Otf/zpGRkawvLwMj8fTTgL/WiiKIjCUu1y9Xkc8Hhfols1mkU6npf9NpnppaUk86QlhS6USent74fV6t5FH3C2oC0ilUigUCgiFQgKfOb5LZpsEHdEHyax6vY633noL5XIZly5dEi9Ct9stPIDRaITX6xWTDjr8UNZMg1OSjLTV4iQfiSyeEkRCSqfTyfFjVLTxd7C1RYTAXZy8Rj6fx+LiItbW1mSgh4uIZzEyWXGH42tSQw9cl9GSyV9cXEShUJARX15zlg9c3FqNA5GL1iOAw1zpdBrz8/NC7K6ursqJUBQmadurvDYsQzKZDHw+H8LhsLgm8XcbjUb5HnO5HOLxuCR+EodEmj6fT46q473FqU632w2fzyefjwm50WhgYGAA3d3d0Ov1uHTpEjKZDDwejwy13Y64o5OAxWJBOByWwY1oNIqZmRmoqorOzk7U63Wsr6+LcISuuIlEAqurq7LjulwudHZ2wu12i7afM+28KdlX5880Gg1sbGygWq2it7dX2G5tR4I3WiqVEoELIbyiKOjv75cFoD1PwGq1CsehNUBhm5M7OucI2Dokqeh2u+FwOCRhBAIBsfPSmppwt+Yuy/HjaDQqsJadFbZZeQ1INhoMBql/eWNT8EMvRx5trtX1U23JRKk1LuF1Z0eD/AnrYh6sSlck8jRsW/r9fpTLZQQCAfh8Pllg/N74XCYAljLsPrD1p/0OWHpQDUk+SCsAY3lD0pWHz/IaspSLRCLSLaJJKlEFANx33304ePAgzp8/j9nZWSEUb1fc0UmAkNlsNmPnzp0wGo1YXl6WjM/RX/bwWTe/8cYbMi03OTmJHTt2SD2oZdrJ2LMtxNAewV0oFLCxsSFQlDcBfeotFotkfq3WQFEU8ffnLqztn7PW9Xq98Hg8iMViSCQS26y4SBhSxkvRTTqdRrValUm/SCQiCZOiH7a/0um0IIdMJoMTJ07IWYxafwKDwSAqtp6eHhFX6XQ6XLt2DalUCkePHsWuXbvkgFUAGB0dlffB9iS1+M1mU64BEy3l3KqqivSXBKqWiaf5CQVBTqdT2PvJyUlkMhkhiEnmaYlKniGwvLwMr9eLffv2wePxSKLQTnrSypxDS1QhFgoF4Wv4vVF8VKvV4PF4EAgEsLGxIcKpxcVFJJNJ5PN5jI+Pw+l0olAoiJ6kWCzipz/9KYLBIIaHh/HJT34SyWQSMzMz73ua0q2IOzoJUIsObBmL8mbhnLz2ZuRuMzc3J6YN7PFyR6BOnrssD6qkIw/ZYiYebYuHAp96vY5IJIJYLCZtpFQqJSo67sBMGISJ7OlT41+r1aREoIiGTkGE24SwTFKso7WTctydeb1cLpfAX7b7OJVYrVYlcRJZMOk4nU709/fLvw0ODgrxScHL5cuXMTo6CpfLJVOTLpdLiDSbzbZNbKTV6NNQhApEJp8b+/l8HmXTLGcAbJMUO51O8ZbgdCMTES3GSIz6fD6EQiG5fnQzJmm3ubkp74teAzyNeGJiQuC89lxJKhm1IjAAYpGWSCSQTCYFeRIFqqqKdDqNxcVFDA4OCjHocrnaSeD9gr1W2kc7HA7pLzudThlw4e6SyWSwuLgohA+Pytb2zP9/9t49NtLzOvN8Pt5vxSKrisXinewmm93se7e7JVmSLcdSrIm9EYIEUWY80Dgb7KwnMwkCL5DNbAJkZ40BJtjMLBwI8ACDLOKZ2Y2zgDHxOAlGsRNHsmypW93qe7PJbt4vVSSLrAtZvFbx2z+qf4dvUXLikdi2JecFGt3NS9VX3/e+5/Kc5zyHMBR6KuBcKBSyPBdjA2GGmjclLsLLVCqlfD6vxsZGSXsjw936fjqdVjweV0NDgxku8mA2zv7yEMYPwpJ7YMjBaY0OBoP2GauqqozxyGcnXWFz+r5vAzeoeiA2giZ/dXW1zp07p2w2q5s3bxpukU6nlUwmLZJxxUhcAVBKZFRHeEYcUPf5Et2QDrmRAsAaFQS3pwKAUpL9rqu3AE7T29tbUiUBC0LujWvlPT2vKICaSqUUCAQsSgL8dV8H41dfX2/dh5IMLE6n00bddqdMb29v6+tf/7rW19cVCoXkeZ7a29ttlsFBrx/ICHie939L+oykRd/3Tzz8WkjFuQO9KoqH/KLv+ymvaNK+JOlnJK1L+pzv+28f+JVrj4gCopxIJJTJZCwC4MaySXO5XImMeEtLi+Wi+w+WK/gJvTQUCikWi5lXYZPhmaj54u3xGPDfI5FIiSpQeXm5otGootFoiYfjMCNmSk7KYXf7AXjPra2tks5FKhQcDGlPKgtDgdFBWBRq7uTkpOXD6XRawWBQhw8fViwWe8egjkQiofn5eWUyGWMMUm1BwtutXvD+kswDYkDo5SA1ILICAOb5UYlIpVJmPCk/BoNBzc3NWSkUD0vFA2NHPu+Kt2xvF6dFI3YC14HX2N7eVjwe18LCgn0mUH2MD3uHigXPaf+CyQinBONHOZIID2KZO9HpoNcPGgn8kaSXJf1H52u/JemvfN//N57n/dbD//+vKmoODjz885iKwqOPHdQFu8vV8CMPw8OkUik7bFhmLK8kIwDRtuvWowFxAKYo4+3s7CiTyRho5OaZLMp8Ozs7lg+PjY1paWlJkUjE8sxIJGKRDA8erKGxsdEUe/HGrpcnXA0Gg7Z5uR8oE0GIkWSkHAgy4A0QrY4fP67Z2Vlls1l1dnZqfn7egL+ysjJFo1EdOXJEsVjMogxJisViVuLq7+/X2bNnbVYjJTRXhi0QCGhnZ8d09/hMtBRXV1crm82a9/c8z4wOdGoMBcZkfX3dSGL0bHR1dSmVStnhIxVcW1szopZLJyetw4gyXwCjy/NfWVnR3bt3jYiWz+ctWgA4JIok5QGMdbUSJNnrJRIJBYNB+0zc3/7+fh05csSYjGBWPzIj4Pv+a57n9e778guSnnn4769I+hsVjcALkv6jX3Q/b3qe1+SV6g4e2HJLWYODg7p7924JJZiNUl1dbQAeG5hSHD+HBybvxqK7fQIcUpByGHJNTU0KBAK2mcjjFhcX1dTUpKGhIUOd3bIj70sYGAwGrUkG8oo7a5CSG+BaMpm0KIMNAv03EolY7u+Ccu4AD167tbVV1dXVmpqaUn9/v3K5nE1qxqj09PQoFotpY2PDPrskvfXWW8rlchoaGtLZs2cVCARUW1treAagHwcWzgLGz03hoFcDshKNgZ+AhRAtIPWeSqUUDAa1vLyspaUlhcNhUzKGOwEZi7C8qqpKiURCs7Oz6unpkSSrdLjaFFx7IpHQ/fv3lc1mrf8AZJ+9xLMA+3Cp0Ovr6zax+OGZ0ubmppLJpDo7OxUKhey1wE0CgYCi0ajeeust00jA4B/kej+YQCsH2/f9uOd50Ydf75A04/zc7MOvHbgRIAzO5/Nm+bGcrn6c53laXl42vQBpr8wG6s+DBKUNBALKZrNW+4dURGML3mp1ddUGS7iNPqDQbgstoSchPP8ndQiHw4pEIspms4Y/wAlweQ7IfqGb6JahABa5XknGO4Dh6JYHSYVaW1uNBitJs7OzmpmZUWdnp1paWqzsx2FGYJRwFVATVhwNNdxrPL47WZkyKqIuGBx3UjMelgPilhOJDNzeh0QiYZgMUu2uQhD7YmFhQZubmxocHLRSoysDzyFeXFy0Q8k9pSqCsQDvcFMLwEmqMPX19aqvry9JR4kaUqmUIpFIifJSKpWyyUgVFRV2r8G5DnI9CmDw3UYQvQPN8A5g7gDsLTwsgzbd0B4PAnnHXW5Nm6YVPBH15vr6eqMGE1lw4AgrAYSkPX4/6QTXQo2ZzR2JRAzcCgQC1nq8vLxsYByfEWksNjAlvXA4bCG3m+8T4rr/JkQl9OQQgGxvb2/boJHOzk7FYjEbhnHkyBFtbm7q7bffNt5CXV2dFhYW1NbWps997nN66qmnFAwGjTREmoLRQnfPbW12Pf7u7q5xNajGYKjdtM6VX3NlyuA+UOHZ3Ny0xjC+BzY0PDys27dvGybT29ur2tpaAxQXFxdLJlDV1NSot7fXNCNgbZLS0FnJc3L3HdyIXC6nuro66y5lEdW4wCYDYqi+MJOgubm5xJEd1Ho/RmCBMN/zvDZJDFObldTl/FynpHfQnfwDmjtAjd317C5RhYfLiHHW1taWZmdnVV5ebmpBLjtwdXXVGHcAXFBW2Yh4JDYj4B/pxMPPaZx3DEtZWZkymYw1nwBmtrS0WP6HICoHBqTZpf6CRJN/wzVA4qu8vNxq33AlAKoo7THKC1k1qRgddHd3KxqNamdnR6urq5qbmzNyEPeqoaFB/+Af/AOFw2GlUikLd1FLwhAB0kp7uAV5tud51ui1vLxcwtEglMdQNjQ0vANFB7jkEKIwhXYjURJYA8bl8ccf1/z8vImgwBJFcq22tlaHDh2yzwKoWFtba/oD4AGkKRhdDACGCy6E28bOAmdKJBLWwYhRBFwkOnGj24Nc7+dV/6ukfyLp3zz8++vO1/+F53lfVREQzDwKPEDaU2vZ2trSzMyM6fUBDpL/J5NJpVKlk9AI28bHx7W1taX29nbzKLQOE043NTVZuQumGCEoXk7aU6tZXV3VzMxMCT9/ZGREVVVVOnnypFpaWiTtzSSsrCzKb5eVldkcPzYNG4ZNRbcjpTZUewgZaTKidZj3wHsghgJ+wO8SDeDZ6JHwfd9CVQ45Gx5wc2JiwrwjBgiPhxF2+RFQbPncHHr6JxoaGhQOh618BrCJ2pLbBuwi6eyHVCpl/RDwECgzIhjreZ66urpUVVWlxcVFXblyRf39/fr5n/953bp1S7lcTi0tLRYNEHm47EKiR54D1+OmcjAjubaGhoaSej/Xl81mjQkJlpNIJLS4uGhgpitnd5Cr7O/+EcnzvD+W9IakQc/zZj3P+xUVD/9znufdV3H2wL95+ON/IWlc0gNJ/0HSrx7oFTuL1s0LFy7ozJkz+ta3vmXtoYhL7OzsaHZ29h2pAOh5PB7XjRs3dOXKFY2NjZUIZ0gqiQA4sHhR/o8hwBMQneTzec3MzOjatWvyfV+nTp1Sc3OzNZmw8ckx3Y0i7c2243WpUOzsFOcFLC8vS5JxAsAHAJGYmITn53O58/xcNdx0Om0DOTmsNPe0trbq1KlTJoISiURUV1en+fl5bWxsGOGGjkFJ5vGJlLhWjBlDXPj8kJw4PDT6oCREWRMdf0A8oiBwChdshd1HpMYh5RnQK7K1tWV03vb2dgWDQWtBBwR1c3/Ss0KhUDLmjOeDEVhbWyuRS+vo6DDcRZIpIAGa0kS1vb2tmZkZLS0tGSuRPpmDXj9odeAffp9vffJdftaX9M/fz0X9oIuHBDr+2GOPKZlMmpenZOPm7O5KJpPK5XJWwoH777auSnvYAWEtkQLgF2q/NO8QorsHEGIRXHRAJuTPiFzwmngxPgehJXwASWpvb5fv+1peXjb+AJRkt6oBiEZkBMLvgpKbm5sGqDLHkQ2fy+XMyLjaAHAAKP3xOV2NALdEK+0Bsnjmrq4ua4AilOb7vM/y8rJpLBC9IOvGa7uVFsA60H0ATd4fI8d9hda9vLysb3/72wbCkQK494bPTfclQB2pEvcAw0SbtOcVhUnQfJidnTVsg7C/ublZra2t8jzPUsdgMGipDJ/3oNcHmjFILRtyx5kzZ/SNb3xD0l5LK5p/5NMuYAha3N7ebsM4pD2GIX3xgDuUmQjR8TaSLHQmT8TbBYNBs+A0/FRUVNh77R94Qe7PhpNkoTlaB/TvQ1eGBYjXomuO9lcOBiAmCkyFQqFEmQjCk1spcScdYdRcAU9Yc0Q2qVTKdByi0agdcJB/t6zrVlJIpQif3XvY2dlpnwFgFeCU8JzX3d3dtRye/N/VeiQcJ7Ug+tja2lIkEtFTTz1lzxkDzTOCtJXP55VMJjU/P2/UZXAV3gs8h0pTOBxWPB7Xzs6OjbUjkuO5Ywz2N0TBb4lEIhYlHOT6QBsBSDWrq6taWFiw8BeloEAgYJtL2qPfArLwN+APm5xyGjVfynNswMrKSiPwEG24/+a12UyU6Orq6gz0Qf8/EAiU/B5iJWwEN1xnc/EZoTX7/p7YCWCYVDQefAaXiMJGllRSg+dgQnNNJpMlG5yDiUcCP8nlcoYruHqJCLvAp4CpR2MSGIQki0wkWXMQ1Q8YhO9WziVlcFmRiIZEo1FLKzjEsCO5v9lsVul0WuFwWK2trUqlUgaSkudTWaATE3lzKgXr6+smsILUHVUFUqLy8nINDg7afIO6ujojtPGZcGpEi7wXJWkqUge9PtBGgE1YW1urrq4uEwTBOxOuEz6zUZqamqzeDUuNjr1CoWB0VJpFyNX3d7qtra2poaHBuvgI8dlA1dXVamtr08rKiiYmJtTQ0GBkHMQoYrGYkUuobBByuqBfNBot0eknOonH41aVcD07G58SIjwKKNZ4LbdSgSFwqdZwFmpra7W0tFQiRybJGo0IiysrK98hAOKG/26+zjW6TVl4RYwOkQu5NtFZIBAwBSBJhoMAsmEEGAPHwaZPAOINjU8YOBdTkWRjyBGHnZqa0vDwsBkwIqxoNKqenh4DUUmb3MqMJGsiowJFMxvRCPvLxVMCgYB6e3s1OTn5SCoEH2gjgEemNEYXl9sJSCjLKi8v9slTVquqqlJDQ4NxwCH8kBe7IaZbiyZMm56eNmIIeniEuzxwvMXY2JhVGdzDRc89jT6g4q4wCVwGAELX0JByQD6pqamxshteCTosxgdUmgOJlyXdCYVCdj3uZl9ZWTGQFcNBiuB5XskwTtIeNjz3H6EORpKD9DNNCHIU70MEhVd3cRt6MUhxYFpizBGbcUVNeL7379/XwsKCBgYGTEuC9yPsrqysNOm1u3fvamZmRocOHbJUK51OmyYl8yBoGoKfwfPDicDHAIjFOdEbgWQbYO3p06f1ve99zypAOLuDWh9oI1BWVmZTYjhkhNUcrGw2WwIKUtppbGw0jjusNLchyW0LJRzGkxHSIkk2NTWlaDRqr8VGWFxc1PT0tKamphSLxTQ1NaW+vj61tbUZF4CSFR4U4IsD5oa9gG+U96Q9uXVyd4xHoVCclrS1tWVhOB7NBRnJlQE1OfQArDU1NbZR6dnf3S3OZoxEImYkMV5gF64oC5GUC8bRow92434PQ1coFNTY2GgeHWxHkn0fbIDPzbPJZrM2XBb8h++n02ktLS3pwYMHikQi+vznP6+jR4/qv/23/6aamhr7OZieiURC09PTWl9f19GjR9Xd3a1gMGj3jFAd7w+TEEzD7S/h+sGiEomERVWUVl0jPz09rSeffNKMNJ/3INcH2ghAO11fXzcwzL1ZWHZpT4CEh0srLb3oVAMymYySyaQdUkJvQDLIK5BRSA2glXI9RALJZFLt7e164YUXNDo6qrW1NXvI7vw+gCEEUQijiWo49HhBPisGgyoIB4OGGcJRSljkonhvvDB5NXMTSGug9eKp3c8LFwHE2i1DgglwEBBOxRNKxS5A0ho8aSqVsuoFByIWi5W8PwQnUjS32QcRGHe2IZLmmUzGqMB1dXV6/PHH1draagxBHMDu7q6BrQsLC7p37552d4uipW1tbdZmTGqEo8CYu81PkJd2d3etVEjFCDIaRsCVKHMxExdb4n4d5PrAGwF45XTp4fU5OKDHkkyIQ5KV9dz8lFwaPvfCwoK2t7dtRDn1fxeIo+6fyWQUj8ftgFHmW11d1Sc/+Uk9++yzamxs1K1btyz3d9WK3E3M4eHr1NIlWfRAqkPaA3CG16N6gOeBlkxEAw+AgwSQhYd1DwIj1YiEqGUDXMI8DAQChqRTQnWNF5sboI0QGbIPEQC9GrwnQqNoRtKhx2HjXhF1YKxhdVJmjMfjymazikajOnbsmI4cOWJK1FLRECHMigzY4uKiYrGYurq6Svo+wFukPeYm+4e0jT1H5MPhJ1Xi+li8Bp+LfQaRDGD4oNcH2gjk83mjxzY3N2t8fNxuPqQft2uQjcbNJGx1KZ2SrBMObcC1tTVNTU2pUCgoGAyqo6PDxDkBc/L5vILBoMLhsB0OJty+/fbblvu6E39oBV5fX7f2ZBBrwlgiBxR92ByAnUQOAE8uockNUV2gSlJJWzINNO7gTve1oC3D+sPT8zN8hv3vS6ogyQBNogCuhfSMNIv3oDzGa8F/cKM5/ibCwZjxWbk2SSYEwuDV9vZ2IxI1NzdbKzfl2OXlZW1sbJhSklv/B0dyDTCfhc/GH5exiC4FgK+7F90qAZ+L6IkxbDs7O4ZlHWRL8QfaCGCZaTcl/2KzSHsVATYneEB9fb1tdryKe4AQoXB7CTj0eCgktykjEha6lNba2lpNT0/rwYMHOnr0qE6dOmX54n7RCfJJt+nIZSq6hBw6/tAcwMOD/gNIuh6Kg4On5h64B8k9tJQzXXEVclWMA+U8z/M0Pz9fcvCYKMznoY2X6UxbW1sKBoOSZAe4oaGhRIYNjIRn6KZDkuweuiw9IpOysuJQD0p6/f39evrpp02UhD3U3NysXC6npaUlraysKJVKqbKyOJfS5Y4gRe42c2GwuK84FXAC7iX7yy0ZYjjcdArDQj8MnycUCimVSpkhcSPc97s+0EYAJJupLX/xF38hSca9BuWVVOIBpT2VHYAxrLzLbsObSLI+f9h9CIbyfgBEeDA8fnV1tTKZjObm5hSJRLS0tKRYLGZklnQ6XULBLS8vNywCQwNKjfcnpGRTYnBcPMINUTnk4AUw/PCueCbSk/1dbW6fQm1trTY2Nmw+IAcT47u/cYrNDGUXA+h2PvJsXE4Hns6t7rjqTxg53/etKQtmo0vzXl5e1vLysvr7+3Xy5MkSnUa6/ND8w8gz6IWfoeTJIs3hM7gRBHvDbbYi6uRZuIApnAnuNyViDAI4QSgU0vj4uL32Qa4PtBGAhhuPx3Xp0iUNDQ1penraAB6ANUklDSyE7yCt5NYAgG5o5lpjSfbACN/T6bTm5+eVTCbtsFRWFodkDg4Oqr6+XhcvXtTjjz+uSCSixcVFU8XFe6+trVkO293dbeUhHr67qQG9CH8XFxeNjIQBIBKgugEAiJFCmw/hVMJxBEeITtwKBeU7SrAucQfDSK4ryTQednd3bUYAYCHXgSHkPfCqHBZ+jjQI0A6AraysOHSGoauzs7NWcaiurtbCwoImJyfV3NysJ5980j4zXIpQKKTt7W1T/y0UCiUzH6EPE9JjrOhKpNSJ8eL6SRGJDtljbmrA83Ujp0KhYE4BsLaiokKxWExLS0sqKysrIUYd1PpAGwEeNvlqR0dHCVrrenJuHAo+eEasPcAWdWeMhJtvEwlQCZBkIqQwFtnEPFxUdgYGBuT7vq5evaqlpSXV1dWZAAeeu7x8b4xWMBg0r0G3HdeLZ3QjF0Aml1UolWIhrs5/oVAombSDVyNHB0Ql1YId53nF1me8I5/XBQAJ9UkxkFvf307rpiFUXTAG/A2WQBjNwWKGRENDg4XyIOhEX1NTUwoEAjp16pSJwXItbjqBwdva2iqpCOFAcApUofhDSI4hJLRHtm7/Yd2PFfi+b7wNvo/hY0/HYjGjKK+vr5cAiQe1PtBGgCGbAwMDOn36tOLxuKqqqkqm6ewHUHgI+9MC2IFECW4/uCQLC8lb8Qb+urzsAAAgAElEQVSg8IyydkM+DnZZWZmxvVAtphGIaAVP4vu+tStjdGpra9XZ2WnIOGGxS+LhILoqOgBrsAP5PGACGEJYf1QCoP+im8Cmraur0+TkpK5evar+/n5dvHjRNjIlOajLYBYuUIdhJtXx/b1WY7dz0KXIcp0cDg5oRUWFtWRjAGALIidWUVGhY8eOKRQK2QHHUAK00VXI/WfsnNuxSbWFhiH2CA6HvcRzcPUFuN+8hrv3Njc3TelJkkUf3Jvy8nJLvRCBxSBimA5ifaCNAB4nm80qHo/bxBfCUg7g/t9x59a5LZ9w4LHI7iRiQlm619yNRNjtNpvwEDl4hJhsjkKhYFjB5uam6uvr1dbWVtIbAMecdAWRULroXIoviDKfkToztXUOEkw8jB8H1q27Y8Bcj4W3jsfjNhKrt7fXmJMMfN3vqdxNC9jIvXFVgngOeEXq6WAdOzs7CofDZsCQNaO7zj10ktTW1lbC74BVyWfkdbg3gMDwK3iOqEHhWNhbGACe/36cAQq227npPqvV1VVNPpzpyAIApi8ll8tpbm5OFy9eNEPHNOqDXB9oI+B5RbmpqakpVVRU6NOf/rSi0ajlxtI7GVY7O8VZhY2NjTZvjo24v6xDPz+elxyNXnhyWWSvKysrDVxi4fXZYC5QR7vo+Pi4RkdHlclkdOTIEUtxXJoyBCC8oQvCSbKcm1AfBqRbnyeUx+u7AiVsVjfH5Xp3d3c1PT2t27dvG3A2OzurK1euqK2tTfX19aYjwPWCGYCT4DHdBi9JBhxieDjE7kZ3MQLXA+NBGXJCZIFXJzVobm42lF9SCYWYdAR+BtEdk45ZLqKPoXOnI9O56KL+fH76H9h/s7OzxkHY2dmx9yCyY/9lMhndvn1bL7zwguEyLs34oNYH2ggQYsE0Y+bd6OioWXiXkCPtlb7i8bgJhFKGcju/ODiSrJyGV0bUg1Ii5aOmpibjibOhiQLwUnT10Ryzubmp3t5eNTQ06Pbt27p8+bIGBgbU0tJSkq97nmeEn5WVFW1ubioajRp675b2eO2ysqL6D2EnJT2uyU2NABrBP/CYNBvduHFDw8PDikQiyuVyJghy7949dXZ26uTJk6qpqbHIwiVjYWCICDC4GCPatfmslOC4BvJrdBzcfgEMA9RnXhtAkfzfVTRyeQg4ErcbkCjS5QAQ+RHR8Xnc9I8/RJK8NqBvoVDQwsKCxsbG7B671Sj6FHiWtbW1ikajRt4i1Tno9XcaAe/dB4/8n5L+B0nbksYk/bLv+2mvKEs+LGnk4a+/6fv+5w/8qh8uQuBDhw7p2LFjdojxkoSe7o0mh4WFR/3a84pacC7Qxe/izVOplHWw0eji1vjZMHh5eu35OTZGMBgsYQUyPPPEiRNWqyZ1IH92+Qtc5/6cmWiAHNcd/w24h/zX/vZqmILS3mFFsPPmzZuanp62jkMiBRpZZmZmFAwG1d3d/Y5IZP/i+tw+DRcwdBWEeB3YkHhgDqFbs+eeE267jUjus8FTu7wCujrdmZNgQkSCLkXYrfWTLrm4h7vnXMl17u2hQ4dUXl6uq1evlnh1Nyqltfrxxx8vwYHe7Z6+3/WDRAJ/pHcOHvmmpH/p+37e87zfk/QvVZw5IEljvu+fOdCr/D6Lkt/W1pbm5uZ09uxZs9gcMJdjz0qn04rFYjp+/Li6u7s1MzOjuro668RzaZs8QDZEPp/XwsKChXF4fshFHFTyXVB+FwBko7sz9GiWcVWGeX88hts0AyqOd3QNH/cA3T+YcS4gSLnJzct5Xw5EWVlRb//y5cumdus211BNSCaTGhsbUzAYVEtLS4lQiWuAXaITzw6Py+HiIBBNkD6w3DQAw0FYz2vw2i4ngdQNg+J5ns0F4PlisMGLiDjcrxMJujx/PhvPkbTHNbZ83fd9w5n241VMk8YptbS0WHlwbGzMqh8Hvf5OI+C/y+AR3/f/0vnvm5J+4WAv6wdbNOysrKxYk0hbW5sNpaAshnINq7a2VkNDQ4pGo6Zky0N2mWB4GtcbkONxUACmQMjxSBBXtra21NraWuKxPM+zEhq6fJlMpqTVt66uzuislJzcdl9J1kzC5sNIUfbCoLgGB7kr1IwpH3JQub6amhptbGzo/v37ymQyamlpUVdXl3VrMkUIDYarV69qfX1dn/rUp6we7zLhOLxra2slbdGE2hCTAORITzAcrk4hXyNS4Joh2RARSrKqAodVkkWIAIvsE/ABF1B0S62uN+f9uSYMC2AiESe0b3QhFhYWNDc3Z70Z7gqFQiXRUDgcVk9Pj1555RWTgDtoopB0MJjA/6jiTEJWn+d51yRlJf2O7/vfebdf8g5g7gBlOjj9gUDABEPIFSmBuUMfCFknJyfV2NiocDhsIV1zc7M16HDDXQSbw0UdmVAdpR8OKptzdXXVFJA4qC7lGGZYU1OT/YzneVYOIiXgALl5J0YL3gIbHbANwEuSpSCudoBUnJNApEDeTLltfHxcN2/eVCAQ0IkTJ9TY2Ghjs6Q9uXdmLE5PT+uv//qv9dhjj9kgTQBVFxyU9oC/QqFgG5zrB3vZ2toycRLCetIt8ASetdtrwKJLk7X/96mWQGMGF5JUokHhsiAxsqRapBdENG5fAFGTu18rKyuNmCTJiGEYJaIs3/fV2dlpCtrt7e26e/fujwYT+NuW53m/LSkv6f95+KW4pG7f95c9zzsv6U89zzvu+352/+/6BzB3AAS/pqZGXV1dtikaGxtNUZf5e24vOg+IzQm6T2gJA4+SElZ+c3PTctn9qLubv3ueZwaBr7l6dRwOl2Xn9qATvrqNIm5I7Yb7bDjSBw4DYivw8OFA4DWpoFCqA5Sju29ubk537txRWVmZBgYG1Nraapt/Z2fH5jB6nqdYLKbu7m7F43HdvHlTqVRKH/vYx9Td3S1JVnbDMEl7BxIgEE8Lb4DnCi0XbANP7RoOl7vv4jkuCEk0x0HjNfn/7u6upViE/JCzuN5CoShUwoAYtxvUzeV3d3ctWtyPEdTX16ujo0Nzc3MqKyuzcJ+0DGNUUVFhYjcVFRVKp9PKZrMlRuWg1ns2Ap7n/RMVAcNP+g93qO/7W5K2Hv77qud5Y5KOSLpyANf6rgsRRsCyQCCg5uZmTU5OWm6H2qwLggGiJZNJK+8RAqImW19fr1AoZAaioqLCcnwOnOvZyBMJbRcXF02hx1UBBmcgxCeHpNbMz/EzLhhFXwREI0A2rokNL8kOBig4G8oVLgXJhmpcVlamVCqly5cva3Z2VkNDQ+ru7tbubrG3fmpqyvACKg7UwPFY5eXlun37tlZWVjQwMGDhNeIn5OUcQEhRhM2E8xwsWo6JViix8nsYIzATDjefVVKJccCwuyXYsrKykl4Q8ncOZSaTUSqVMuYm7831uZUQsACMHNeLVBnCLJ5XVGpCQoyOUCI23/fV3t6uiYkJLS0tmTDpQa/3ZAQ8z3teRSDw477vrztfb5G04vt+wfO8QypOJh4/kCv9PosNfe/ePZ07d06RSMQeCA8yFAqV0G0xAqFQyERK8QSUkxChyOeLIhg8ILdiwEZ4+NklqQSHgNnGe7u5r7SnhcjhJgSWSgFNt/znEoJIaxKJhLLZrGKxmEl2EfICUBEhUNpE4gqcI5lManZ2VvF43KTWjh07pqamppLDCTXbNWbNzc2qri5OPsLo7e7uKpFIaHV1VaFQyGY1uv34RCBs7mAwaF6e5jAMk8ts5GvcMwRYQOcB6Sh9umkY1+16bvAaN3IgxEcVmAYj3n9jY6ME28CL8z0iJow5USCkNH6HUi6j3DEe/f39unDhgsrKyjQ+Pm5l6UexfpAS4R+rOH044nnerKTfVbEaUC3pmw83P6XAj0n6PzzPy0sqSPq87/srj+TKHy48MaWvpqYmY4sRHjIlmHyKsuDAwIDKy8uVSqVKwknCeB7SysqKhb9bW1tGCMKj0CHnduFJsg0oyQ6/67lAtdnAbhMKAKUkU8dF6445CouLi3Zoa2pqlEwmtbS0JKlojMBB6IBcX1+3tIgohMhHKnrL48ePq6enx0AqDgyAIikEZUjCbD6rS/8FeMRYcM+YVjwxMaHXX39dGxsb6u3t1cWLF80Q19TUKJVKKR6PK5lMqqGhQa2trXY/Ojs7zdhQBXF7DMBN2COkjhhwjDb7x+3LoLQHeIfQiBtl7e9JwaBg3Nw5CryeJKtQ8G/YgR0dHfaMKiuLA2J3dnb0xhtvKBKJaGxs7JGUB6UfrDrwboNH/vD7/OzXJH3t/V7Uf8+Kx+N64403DDjp6urSRz7yEd2/f1/Dw8PWpDM0NKTvfve7Zunv3bunxsZGPfHEE3aQiAI4rHhzN+RF4ZYNjffgYNDjD4mIA066weGgjOV20pEPu+FwPp+3phZCeN/3NTs7a94ymUyaYWAzh0IhdXV1KRqNmszZ7Oys1tbWFAqF1NbWpv7+frW0tNjGJTXgMPPejY2NFjrv97hgJXhdmG7InbPc7kEikK6uLj399NNGZgIAXVhY0Pz8vIXAAMA0aFG6i0QiFuG45UQwj42NDZsxSKiP8eKzutEXBxtQeW1tTalUqkTPwOUokHIQASBPR2pH2iYVx78lEgmbielGYaFQSB0dHdrd3bWRY5FIxOZgvvXWWzbp6VGsDzRjkAUT7tq1a9rZ2dHFixeNKbe6uqra2lrTsENKand3V1NTUyYxhQd20V6XB05uiEdwF2FfoVAwPXqXIpzP5404hAIwpCbUiMhfMSbwy3l/N1LggC4sLGhiYkKzs7NWe66urlZra6smJiZ05coVhcNhHTt2TMFgUF1dXerp6bHKgyRLeQhbMWiwHiWV9Fq4B8rt7ecAYgiIcjCSrgY/f/g8DAXZ2trS4uKiNjY2rHNOklVJFhcXtbi4qMHBQTPuRGVUd/gccAi4X255lfu0urpaEh24DU/cE7o8STHZby4GBDmJdIGvQ82WVFKGlYqRB5EprcMAiWfPntW5c+e0ubmpZDIpSSWNRge9PhRGACv80z/90+rr61NFRYUuXLig+/fva3Jy0nL0UChUMgxyeXlZY2NjOn/+vBoaGix8hugCgaiystKmw7jMMOr98Xhc+XzeRmsT8rmb0hXwgGtAyI5XA5yj6uDqCoCw895LS0v6y7/8S83Ozqqrq0uhUMg2GrTXsbExXb16VVeuXFEwGFRfX5+eeuoptbe3q7u724hFDLtwJcIklaQleE+3Jg+YB/JNu7G0ZyDwdugVugaViIdcf2dnx4ZzoHlAaoCRrqystKGrlIPBCAB2IXJhPDnUvAZRg7SH5bilRFJJlzVINOeWUDEGm5ubSqVSVjUhunDLhnAgSCc40HSlbm5u2kDWT33qU2poaNC3v/1tI165eNBBrw+8EfB9X6lUylR1Ll26pObmZg0NDens2bOm81dfX6+enh6bAdfc3KyVlRXdunVLg4ODRihi07mEFFfBVtrLMfnewsKC7t69q1AopKNHjyoQCNiEYJdYwgFtaGhQWVmZlblWV1eNLcbhYrOw0UDwAbJQDV5YWFBvb69aW1vt+y53AH7B2tqaZmdndezYMQ0MDFgpVZKFu6Q+eEEXVOSPe18wcHQ1MihU2qP28ozcJiruSVlZmW7cuFHyeUZGRvTgwQMtLi6qtrZWfX19pg8QCAQUDAa1sbGhRCJh2v80O2EkKDGCpWBMMAbk5wCVRGB4cX7GTfWkUpUjDr87sJZIj3vmsgR3dor6gPv5/7W1tZbura+vKxaLqbm5WQsLCzaDYGRk5JHhAdKHwAhIMnWfkZERHT58WG1tbaqurtbx48d1+fJlzc3Nqbu7W4cOHdLt27eVTqfV0tIiz/O0vLyskZERnT592nq52Qx4f0nWaceGgA3GRuvq6tLa2prefvttQ3sPHz5c0lZKqoHUdiAQsDRgY2PDSpVsHDypW6qSZB6vUCioubnZdPCJFFAuGhgYMPBwa2tLAwMDOnnypAqFgpGYODB4fRcTgcjkpgn8HN9DqoxoDKCNn8EbUukA5ad85nme3njjDd26dcvGplNG5P50dnYaAFtRUaHp6WnduXNH0WhUzc3N9v5EMRgxKiyUD91DTrSE8SeCAMAkxeDwASxiBDAUcE34fBxeUqpwOKxUKmVt4fu9eTAYVFNTk32GcDhsugLPPfecksmk/vN//s8lLccHvT4URmBra0vj4+P61Kc+pUKhoJmZGRsxff78eSUSCa2vr6uhoUGdnZ0mJHn27Fm99tprevDggQYHB+3BumCZJMtxediZTEbz8/Pm2aqrq3Xo0CE74GjYT0xMWE+BJMstGVzBRnQbkTho1JcpIxJ90EfPJKDq6mpLL6ApNzQ0GH16fX1d3d3dOnfunJ599llFo1Hb5KlUysJxt/QIuOh6bXJiFyQDlQdIJJXBaEl7rELyayoSGITBwUGFw2GFQiGNjIwokUhoZWXFSFErKyuqqCgO6kin05a3379/X83NzXr88cdLqhhUImjzxTjxb0lm6DjE7ih5SEQ8f16Tz0d04FZxADMZJU6KBOKPDD30dlZ5ebna29tVXV1tehgXLlyQ7/uanJyU7/uampqycuOjWh8KIyBJ8/Pzlmd3dHRYDffw4cM2Rbe2tlbd3d0aGxvT3Nyczp8/r8OHD2t0dFSJREKRSEQzMzNmifGSbh0ZlDedTlsVAKIHYW1bW5s2Nzc1MzOjcDhsOZ9LOUZEUtqjpUoyj8tGdI0Dn2l8fFzDw8PK5/OKRqNGzaX8WFVVZbXtrq4uPfHEEzpz5oyRljAsoPsuKOqWL0HjOeSEuqQY3HPXq1K2k2SGg/w9HA6/QxmJ3PuZZ57RJz7xCaXTaU1MTCiRSCiVSllOnM/nS2Ymrq2t6fr16zpy5Ii6urosTydycRF7F1Dj3kLoYSApfH8OvNuLQNrH4ebg53I5JZNJM3xcL4Cy7/uam5szUHhsbKzkWqqri5OjJRnLcmhoSLlcTtPT00bQSqfTjwwPkD5ERmBra0srKyuqr69Xf3+/kWlisZh6enp0/fp1NTY2qq2tTd3d3bpz546mp6fV29urBw8eaGRkRE1NTcrlchodHTUOgbQ3dNOlzfLg0RHgINBJSDiNvDkAm1tapFEGZSSEQ1wAzq13Ay65LcugzOFw2Kbmjo6OqqqqSmfPntUTTzxhikQuqWa/cXM73qgAbG9vK5VKaWZmxrxmNpstqa3TY1FdXa3u7m6Vl5dbOZVKQT6fN7EP8AOXNCTJ7k9TU5NOnTqlgYEBpdNpra6uKpvNKpVKaWtrS0tLS9rY2NDy8rJVY3p6eiyMB3cBw6DSgn4h3rysrNgWPTs7a/ebtIxn5DIT19fXlUqlJMk6VyllRiIR9fT0qKOjwwDe6upqI5yFQiElEgmtra1ZaXFtbU3BYFCtra327D/60Y8qGAxqa2tLbW1tunfvns1SfJTrQ2MEdnZ2tLKyoubmZqVSKZvSGwwG9dxzz2lyclLJZFI9PT3q7e3VvXv39Pbbb+vkyZNqamrS/Py8FhYWVF5eromJCeXzeXV0dFgpCYSfkG53d1eBQMBoxe6GAWnu7u424hHcfEpGhP1u4w/ei7TDxSekPY17hDU8z9PS0pJFIcvLy1pcXFRvb6+effZZnThxwrwmOSvv6fZCMDcASit5NeWzhoYGLS4uWsmOyomrz0dE5HlFLYDW1lY7SGAbhOnSHjnHTTW4dxgc6vBgD01NTbpz547u3LmjbDZr7EgAVmrvblUAI8P7u6xB2qtJh8BTXGCYa4THsbKyoqmpKdNmOHr0qDFDY7GYgafgC0Ql5PRUD8rLy9Xb22vck+PHj6urq0szMzN64403jJiGIXmU60NjBCQpkUiotbVV09PT1l145swZHT16VOfPn9ef//mfa3t7W11dXWppaVE6nVZ1dbV6e3t15coV3bt3T5WVleZ1KN1B+qC5CCSe/n8IPGwkmokklVBk2dAun5z3ASAi93fzTl5rbW3NNiMHbnd3V/F4XHNzc6qoqNBTTz2lz3zmM4pEIgY2kR+vra3ZwYVCTA+C53mmzgM5B1AtFArZEM/y8nLFYjHr2iQMXlpaUjgcVjgctjIa49sA30gviJowllROOMCImQAUEuo3NjbqwoULikQievXVV5XL5ZTJZIxN6TIZ8Z7k/6Q0RDsVFRVqbW21aAcMgrZvSSVaDqQ6PNtcLmcCM/tZku7/AZ+5HqpC4XBY3d3dxvH42Z/9WfP+R48eVVVVlW7cuGHVl0e5PlRGYGpqSmfOnDH2WV9fn4Xn586d09WrV7WysqKenh595CMf0Te/+U3dunVLZ8+eVVtbm2ZnZ9Xf369YLKaFhQUNDw+rt7dX0WhUFRVFdVtyWxh91MrxZpWVldZqi1d1Q3vSCNh3NDdJKgGeXH47JSSpiB/U19fr6NGjGhsb08jIiHK5nFpaWvTss8/qqaeeUldXlyRZfRmwjE3K32AapBV1dXVWQ8/n81paWjJprHQ6rVQqpdbWVjU1NRkrzw2h0+m0DdMAIAMM5SBBQqJMhtEDMKTWHolEVFlZaSU1PHl1dbWOHTtm4Bkpx/57hwGVSoeakF7RMdnR0WFGBzCTCAxeCAsDQOTnNiu5wCPPGIwG0FGSCZm0t7crEAhobm5O9+/f1507d3T8+HGNjIzo5s2b6u3t1erq6iNjCbrrQ2UE5ubmND8/r0AgoHA4bIehUCjo2LFj+sxnPqN//+//vRoaGvTYY49peHhYMzMzkqQjR45ofn5e+Xxep0+fNpQ/l8spm82qr6/PdP9YRAFsQJhtEGP4OkYAZhnXRG5aKBS0srKilpYWtbS0lFBs3b75QCBgFYAjR44omUyaAbh48aJOnz5tdXQODWkFGxQ5baY4E7pikKAmI9biCrIgf42HvHv3rubn51VbW6tQKCRJhpdwAF2OPYZyfy++VAzX0SiknEdLd0VFRYmsWVVVlbq6uoxtCf5CKZD7i+w8JB+eBZHbzs6ONT1RmqS2zzNAhi6bzRoe5B5wDCrPi8/KcrEKSRb19fT0aH5+XuPj42ppaTEiW2trqz796U9ra2tLly5dUiKROOhj8o71oTICa2trunTpkk2OyeVyqq2tVW9vr8LhsM6fP6+BgQHduHHDyksTExMaGRnRhQsXFAgElEwm5Xmejh07Zg0w2WzW6v0cepd7Lu2p+wAquZ2GeBny3UwmY69FuEr5bXV1tSQcZRPx+xiEsrIytbe365Of/KS6u7sVi8XkeV7JDDuQdDwj/6Z2jxFx6+yhUEjNzc0WloOIg3+k02ndunXLwmj6KjhQUtFYlJeXWwuzyzIsKysrkVkDEMTYESG4bcJEVhhX3/etykI1Au8r7eEtfF40HAAG3V4PkP/y8nJTpOJ9wQKqqqoMsGO8OqkFhCqumb3BPXejEK6tq6tLNTU1mpiY0KFDh/Qbv/EbunDhgjY3N3X06FHt7u7q29/+tsbGxnT79u1Hfm4+VEaAEPH27dvGGjxy5IihwLFYTL/6q7+qL33pS7p165aGhoY0MDCgBw8eWMMN0UFnZ6cBi4wfByTCe0INxQAQ8rKByDXpPJRkh4bDwUFBFQdPR7jJxuYg4W3KysrU3d2tjo4OY/q5UlouoxFSTnl5uQ0lobadSqWMFON5nhYWFhQKhYzLwLVQ5y8vL9f8/LylE+41894caA44ZUL0DNBypDoCPTqdTmt5edlAV8BPQm5IS+9WPQG/AIzEMLipFgeTVIF/83sAs3zNpRwTztfX15cAuG5pkdcjlQCfcBmCgUBAfX19mnw4Hu0LX/iC+vv79d3vfldra2uqrq5WOp3W8PCwRkdHjYH5KNeHyghIxfxveHhY7e3t6unpsbAOCm9XV5deeOEF/eEf/qGy2awef/xxzc3N6e7du+rt7VVFRYUmJiZ06tQpG7hBHkv+X1FRUSIbnk6nS4hDWH6aZsrKyhQKhQzFl2SeFk5CR0dHCROQuQbk5Gx6tzElFArZ5ub9+RlCXqSsXJbezs6OzUTEs6LJiEGAjQgeAJAm7ZGnAO5aWlps0i+NRC0tLers7FRFRYXGx8cNH+HawFbQMlhYWLAyJYpInZ2dBoByrxic4obeHGI4BzwrFydwSUwYEjALSYZ5QGLC4OPJSVP2g5kYM5dMRLTl+74WFhZKQEHuSV1dnX7pl35Jp06d0tbWlmpra5VKpZTL5ayDMplMlnBIHtX60BkBqfhAp6enNT09rZWVFfX19Rl7S5JOnjypkydP6q233tLx48d19OhRXbt2zfTy5ubmdOPGDT355JMlA00RjWTIhFTc9CMjI1ZDjsViqqmpMS+Nt4Cl19DQYFWA2tpaNTc322gyymK8LhvRlbxmI1NVcFVwiUDId5eXl60Zp66uTpFIxEZyMWaM39nZ2bEOR0qPaCQQKgMiUieHCdjW1mb9EIB34XBY29vbmpiY0MLCgnlSt4TGgUmlUhYdEb6jD0CKUigUSsJ6Dr8bbbhUZKIhF4x10wZ3riDXAn3YLcnCeXB7Koi69pOQeD7cf8qqGBqwgLGxMZvVIMmmUbe2turOnTs2XPVRlwZZ73XuwP8u6X+StPTwx/433/f/4uH3/qWkX1FRVOTXfd9/5RFc99+6dnZ2dP/+fXV1den8+fM2YGR5edlEP59//nndvXtX8Xhc58+f18LCguLxuL3G/fv31dHRoZ6eHvOM0t7DpsbseZ5NBM7n8woGg1ZCo87u9ryTc+8nF7kdhxx0wlk8HV6ITeUy2aQ9gQypuLEmJiYMjygvLzc2G9cO5gHo6FKDuaZQKGTsQDoxKVtB/3UPXFVVlQFds7OzmpycLAEHXa1FgFiXfAWtlnIhP8+hI0QnBSC6cqMfaQ9j4H3dXB2pMHcKM9+n398daw+4iJFym4pY9JLw/uvr65qcnLRnU15ersOHD1sK9NJLL6mlpUW3b9/W9evXzQivrKxoaWlJ09PTJUbmUa73OndAkv4v30Uee8kAACAASURBVPd/3/2C53lDkn5J0nFJ7ZK+5XneEd/3fzifxlnJZFLf+973FAqF1NPTY9OJCJmPHj2qn/3Zn9VXv/pVhUIhPf300/rGN75hnYKbm5u6ffu29fpzkPHCeGgORqFQUCAQUG9vryHqlAthyNHNhsfkELOhCWWlUukxEGk84MN7XUL5lfZUhul6hLW2vV2UVc/lcsb+cxFrfp8qQUVFhTY2NpROpw1kdSXAAe3ASijrgYdUVlYqmUwqHo+bkeDr8COkPS0/FIXh2JN+1dTUWFsvpUj4/BhE6NK8Dq3L+4VC6PBbWVlRJpOxVIgoiv6BiYkJAzbdidGIxbgScWAF2WxWi4uLKisrU09Pj6kiuX0CSKzNz88bkQsj7PtFLUEIbdPT0xZZ/jDWe5o78LesFyR91S8Kjk54nvdA0kVJb7znK3yPC9728PCwYrGYdnZ2TN/f84oKuR//+Mc1NzenK1euqK+vT2fPntWlS5dsA8Xjcd25c8e03tjw5PO+79vEoFAopFgsJknGQotEIiUCHm53IJ7LBa4klWwwyo78PoaA3+c1yfMxJsFg0NqruR5yVkhFhPmEyZTBysrKFAwGzYCMj4+roaFBwWBQi4uL5u0RDaHPIBwOm2T54uKipQrV1dW6d++e8vm8zpw5Y+KifB48671793Tt2jUdPXpUjz32mKVC3HN3mIfneSWSbgCShOH8vCt6wjBPgEeiDiovGI7m5mYFAgFTlXZnCUL55XkQEaZSKRUKxeExCKLMzMyUPKP29natrKzoyJEj+uxnP6vy8nKNj49rd3dXfX19mpmZ0cLCguEIP6woQHp/mMC/8DzvJRWVhP8X3/dTkjpUHEbCmn34tXcs7wDmDvxda3d3VyMjIxoaGlJDQ4MePHig9fV11dfXq7W1Vc3NzfrsZz+raDSqy5cv6/Dhw0okEhodHZVUPJi3bt3S7u6uLly4YN1+hULB+P6oEhPWE+aTR7vdbO7UXnJVvKqrVQ/wtd/TQ/GVZNx/d/PzN9FGZWWlWlpaVCgUtLS0ZFgGKQIeeG5uzurRkGiQLccD89pEQOTRHEbKZeTd9BCMj49rampK9fX1mpmZsdIe5TSwhJ6eHiNCQVWmyYr7h8GSZO8h7dGpuU8YKrf9F1YjSH0mk1EymSyh9+7s7CgWi9n7YOjpNCRlo5KCY2hqajLdiomJCRtfzkK+Pp1O68knn1QsFtPi4qLGx8e1sLCgmpoaTU1NKZvNanR09JGpCn+/9V6NwJclfVGS//Dvf6viEBLvXX72Xduf/AOYO/CDrHg8rtdee03V1dWqrq7Wc889p0AgwDUYpz4ej6u/v1+nT5829V5pzxBkMhmr4fq+r5mZGWWzWR09elQdHR2WvxPmS3sTiSm10ewj7U2xwbDwf5dezObnPVmw/tigLli1u7trxgKJ71AopNu3b+u73/2uXnzxRUtB8I65XE4dHR02FxDaNANW3PIZ8t+UwCAcuZRZfh7dhGeffVaxWMxYhchmkVJUVFQoEonYYa6qqlJjY6PJu+Ol3bQIb8znh3mJmAp5viSb5kv4TmMSIb/bO4Hxc42Na5CkPf4GKRn4DyPaXUS/oqJC3d3d8n1fjz/+uJ555hlLOzY3NxUMBpVMJg1onZ6eLnnWP4z1noyA7/sL/NvzvP8g6c8e/ndWUpfzo52S5t/z1R3QGh4eVldXlyKRiGZnZ9Xd3W1lw3w+r6tXr+r69etaWFjQmTNndO7cOV26dMmYd4VCwVo7g8Gg5ubmtLm5qaGhIR06dKgkd8zlcgoEAopGoyU9AGxkOAF4PPgFgGUcKFhzFRUV5oHg5BNNYGBQ9XFr9UQ72WxWW1tbGhwctBp/ZWWlFhYWjKE2ODio5uZmAwPRJoCyS2RB6D82NqbJyUkdP35cVVVVVolpb283DKSmpsbwEddQ0PYM4EcTED0AYAsIrOCxyZ05uC5Tk+gMNB8glPSIEh+AJmVfyrwcZnfWAZEP6QZRGYxKt8Xa932LBkjhuJd0rUrSk08+qVAopFQqZQaDtHVlZUX37t37odCE96/3Onegzfd9oPSfkwSt6b9K+n89z/t3KgKDA5Iuv++rfJ+rUChYY8alS5eUTqetw6+hoUG/+Zu/qU984hN68803rQsvk8no1q1b1iWIYAR5YU1NjU2XhQknSY2NjWpubjbvsr6+bhx7qLCQXfAubCS3ZEaO6QKE7t8uGQeBSrfZSCoSW8rLy7W4uKhYLKa2tjbTvA+FQhYRIHSCPBipAlEFITlGCe/ptu5WV1dbqdBlQ9LZh5fGI0MYCgQCZhjQD0RaPZ1OG5DnMgX53FKpQAiHle8DvILxEBEA8rpkIn6Pz+g+D4yCy9Z0v1dWVqbl5WWbhHz8+HEb0nLkyBGNjY2pt7dX/f392t7e1ujoqDKZjBobG23mBV2qP4r1XucOPON53hkVQ/1JSf+zJPm+f8fzvP9P0l0Vx5P98x9FZeDd1tTUlL797W/rk5/8pPHvqRc3NzfrM5/5jJ5++mn93u/9nt566y2dPn1amUxG8Xhcvb29CgQCxpWnO47DjOdheAYkISSrIbhQAcCru56E/JWmkXw+b63RwWDQvCMLMVNeg0aX/V63vLzcZitms1kFg0ELsxHH5Gtw7F16LUaIVMP3fUUiEcViMaXTaVPDPXz4sDo6OpTP540FSJSCscPouoQk1I+k4kFEmwFa8srKio0kA1QlXCYycL20tNc4xecB4EPwVCoy90ht3GqDSzZyy7UYAu4F7+P7vrLZrNHNDx8+rKqqKqVSKXV3d9v9eP7559XW1mbRCZHXgwcPlMvlNDs7++NrBPz/jrkDD3/+X0v61+/noh7F2t3d1Z07d6xd1vM8LS4uyvd9k7gKBoP6+Z//ed2/f18LCwv6yEc+or/6q7/S6OioTp48qYGBAd27d0+ZTEb19fXKZrM2/gxUGr45lQOYeJJMaowNjYcqFArWy++mAG7ezWu67yHt5cSAipIsZCaNCIfDCgQCisVitinh+peVlVmtPZ/PG12X8NvlDkDkQQ9xe3vbymBra2taXl42HAKmY319vWkFcm1EHxgraM9EHouLi2Y4yfFdHACsBOLQ/kiKn5FUQrAiRE+n05amQObiulxBEYwA+8Wt5BAB7ezsaGpqShsbGwoGgzp06JBu3LihyspK0w78jd/4DV28eNEk3WZmZlQoFIxMlMvlNDY29shbhr/f+lAyBr/f2tnZ0Y0bN2zQ6MmTJ3X69GkLwZuamnTu3Dn97u/+rr74xS8qk8noox/9qL75zW9qZGRE586d08LCglZWVrS8vGwhZSwWU0VFhfXgu5qCeHgiBDgDkqz8RN2eCgEsP4RAoQTjCWkqAkRzqwv5fN7y+f3lR6nY+egKqJaVlSkejxtVluiI18aI8bmYHsShkqTZ2Vk9ePBA7e3tJqdGmzJGsr6+3vAGmpZA4GEgYtC4LxgS14ig2oNhcHNzIiNJJWVUwn6k37LZrCYnJ5VOp616wz2dmZkxjKCqqkoNDQ1qbm5WY2OjRR3cc6kIPFNZaWlpUTKZ1MrKig4fPqzt7W2dOHFCZ8+etTLs9PS0HXae0eTk5A+NHfhu6yfKCEgyRWBUeq9evaqJiQlFo1E9//zzqq+v17Fjx/TSSy/pq1/9qqLRqJ544gm9+uqrunHjhsLhsKHLyWTSWmL7+vqM/LG8vCzP8wzpx/szV4BWWmrMbFLSASINPL/LgwdMdCmsYAH8PEgzKLZbQnNpu9Jef73necpms0YnplkIDMBtUKLtuFAoWLkvmUxaNaCnp0eDg4MWjeA5+Xy5XM4YltT6MWB4ajQhA4GAGQq8N58XFqQri+aWETES1P03NjbMuNENCGcCIxONRq1CEo/HtbS0pEQioba2NjU1NZnhR17s/v372traUnt7u3p7e3X79m0FAgEVCgUdOnRIv/IrvyJJhg1NTEyorq5OiURCiURCy8vLJZyCH8X6iTMCkrS4uKjLly+rsrJSPT09ikajGhoasj6BiooKfeITn9D8/Lz+/M//XCdPntTy8rJu376taDSqUCik5eVlkyCrqqoyoKesrMz62N3NCMEIyShAOc/zrLy0srIi3/fV1tamlpYW85QIfXAo3RISvAQOeEVFhfU4oIm4tLSkTCZjmAWaeRCnaAXGe0IOwquTgjQ0NFi4DUK+vr5uURV5Lsi+tNdiTf5OakF/Ph6Y8qhUnCsZiURKJNipCLjMShf1J0raz78gimpsbLTSZSQSMfk5N9f3fd+YkbFYTJFIROPj44rH4xofHzdjRBdmIpFQLpdTU1OTLl68qEQiYWW/iooKvfDCCzp8+LBVWMrLi+rCd+7cMbXqu3fvlnQZ/ijWT6QR2N3d1fj4uOEAzc3NamlpMVJLa2ur6uvr9Y/+0T9SfX29/st/+S8aHBxUOp02NWLf95VIJHT9+nWbWYBXhVW4n/SDl3UR962tLSUSCRtj3t3dbS20TU1NikajJQg2+bDLFaDNGXUeGos8zzN9wVQqpbffftvC+3A4rPb2djM4gHQw5lzeve/7NqjDBSGJSPr6+tTT0yPP86z6AJ2WMWHb29tqbm42owKmwABVugi5JkA9IimiHqoDRAAcYIwh1+UyKDEKqB5RrXENAOQjV2GItt+ysjKrUNAjQCQRiUT0zDPPaH19Xffu3TMD8dRTT+nxxx+3iG1ubk4jIyOGreTzeQ0PD//QiUHvtn4ijYBU3ChMDSorK9Orr76qwcFB2ziEfr/wC7+g2dlZ/dmf/ZmGhoYsDOSgENIdOXJE58+fLwH9AOFAqAkJeY/NzU1NTk5aOhKLxUyVt6KiQo2NjTaVCMTbPZzuRmez47Eh1rS0tCgWi9nglW984xs6deqUPvrRj9oMPEmWauBtCdM5fBggwvrt7W3LY8FYoEu7RCY+9/LysqUWwWDQfpZKBAKx/B8MwhUb5YAh/AGYyXLvD4aSigZYAl6Zn3cFQUgDIBqVlRWnNNEZWltbq8XFRU1MTFivyPPPP69QKKQ/+ZM/0e5uUQfx5MmT+tznPqeWlhZls1k9ePBAU1NTWl9f19jYmJLJpCYnJ/XgwYMfOjHo3dZPrBGQisDM9773PTtEgUDABEgZB1VbW6tf/uVfliRduXJFhw8fVjabVX9/v44cOaJXXnlF6XRad+/etWGZUhG1p1xEq66bU+NVMpmMGhoa1NXVZah9XV2dWlpaTO6LjULdGykugDA8NJ4R5h/eFCJOT0+PTp06pWeffVatra2W0jQ0NJiXBHF36+uAby6wR/hNCsXrhMNhMwAu759QWpL92+1qpEJAg5AkC+XhLNCl53ZVupwBjAAenohEkoF5vDaGinQsmUwa1uN2a4IprK+vKx6Pa35+3piTn/70pzUwMKCvfe1r2tjYUDQa1eHDh/XZz35WnZ2dWlxc1PDwsIaHhw3zoYV5ZGTkR4oDuOsn2ghIRULKd77zHfNQPHiorOXl5QqFQvr85z+vl19+Wbdu3dKxY8c0NTWlvr4+vfDCC3r99dc1NTWlpaUlZbNZ8x4cQkJASeZd19fXNTMzo3Q6rc7OThv/DZUYspErkOmq6XCdhMAYGDcnBncAbNvd3dXJkyetjk2pDu+Pii4HghFrhOB8Fq5hfX1dq6urVmqkxAfll5/nc5P/U2ZE5SgYDNqQEbfFGuPE72Hk3LZe954CCmI097P/+H1+Fo2CxcVFm/rr9mPw+4CR0KkrKoqqzn19fXr11VdNJszzPI2Pj1uJM5FIKB6PG2cjk8loeXlZ169fL+kw/FGvn3gjIEkrKyt688031draqrW1NR07dkyBQEBLS0t2mBsbG/UzP/MzunfvnsrLy9XZ2anXXntNL774or7whS/o9ddf1yuvvGLlQw4oYBrA2Obmpm2OeDxustWE3XV1dWpubrawmEMt7Q37cMtfbHLXIJBO1NTUKJ1OS5IxHaEzSyqRAl9dXTVSkdu7T3ch0QtgHIAjCkfu13d2dtTR0VHSOozH3djY0MrKik1CIqqBn0947o5vg9gFZ8HlCHDg3fIg94N/03fgRi68Dtct7UUe3HsXLAXILC8v15kzZ3T+/Hm98cYbunr1qkKhkMrLy9Xd3a0XXnhB586dM2Uknvfc3JwpWP844ADu+nsj8HDF43H99V//tZ555hmlUint7u5qaGjIvILv+zp+/Lg+97nP6fd///dtSswbb7yh9fV1feITn9CxY8f0la98xbrDHjx4oMOHD1u5DU9A9WBoaEhNTU0WqjY0NKilpcVyV3dTU2YECZf2VHr4w+FA8INcGrJRPp9XLpeTpBLpLDwjWEgsFrMJuuTf0h5JxmUS7lfXoS9C2qM4c12uR+Zgu+VOlxbM9blNWeTpHEwMp8u85L7Al8hkMjZNietzZyYgXAJzkAjM7UHY2dnR0tKSCoWCTpw4oU996lO6c+eOrly5omg0qtbWVi0tLens2bP6qZ/6KUuBaCmem5vT7Oysrly5Yj0oP07r743Aw7W9vW2MQsZkRyIRxeNx7ezs6OzZs6qvr9fHPvYxNTY26uWXX9bU1JRCoZD+5m/+RvPz83rxxRf1a7/2a/ryl7+s0dFRXb9+XRsbGwoEAsYrR+TEJQ3V1NTYLANUfCDs0LobCATU3NxsSsBUF9z8VVIJmOdy3Ul3aPvlYLjTlnmtxsZG61vggKBJQOVCKo4Mw6tVVVWpqanJyEHpdNqGhvA1SEwQgUiXpD2DRhjuRjzQs13MYH8LtQsCYijhclCVAevAmAC6NjQ0qKGhwTw+4iVS0ZBNTU1pZWXF+iPGxsb0+uuvW9qWz+f1+c9/Xp/+9Ket8jQ6OmqGJh6P6+bNm5qcnPyh6gT8oOvvjYCzfN/XrVu3VFdXpyeffNJKOsh5E9qeP39e/+pf/Sv9wR/8gebn53X06FFdv35dyWRSzzzzjP7xP/7H+spXvqIHDx5odnbWSo7d3d1qbm424g8CHkxBTiQSpq+PdwQhJ1/maxiQjY0Na+jBs/JZ8Nxu2zKlQ7j7HCxJFl5ns1nV1taqqanJ5gkQHoMvAOohQILnTiaTyufzlhbgFdE2RNYL8hREHZfe6yLmfN0N+/m6tNeSvb/e77IrwSogB2UyGW1tbam1tbWkAgGVFyOASvXMzIzy+eI8xUwmo3v37mlzc1O9vb3K5XImhNLQ0KDp6Wl95zvf0ezsrJqamrS5uampqSmNjo5a2vHjtrwfhxKF9wj1BN7rqq6u1k//9E/ryJEjKisrU0tLi8rKyjQ4OKgzZ85YmPi1r31NX/7yl41Rdv/+fdXV1enUqVP67ne/q9HRUTU2Nuq5554z4JFBqaFQyJpHcrmcSW7HYjG1traaFycKcNV03N4Ct3Mwn89bM4qbwzOMhQYeSYZUI62FlwXIwvsSjTCGa2FhwerkqBZzmFxGIbRhDIArN8bYNaIZDqsbKlMZcLv2XNEVV96d/J6f3djYUC6XM/5BoVCw6b6AjWg+MrU6k8kYYLe+vq5bt24Zjbi1tVVVVcVpz319fcpms+ru7tbv/M7vaGhoyMDLW7duaWJiQktLS3rw4IFeffVVUwz6MVhXfd//yP4v/n0k8H3W1taWvvWtb2l3d1enTp2yQZXMoadX/ud+7ud05swZ/af/9J90584dnThxQpcvX9arr76qoaEhTU5OmujksWPHTF4LWiw1d4RN3DB1d3fXSD3o7rHZ0um0tSbTvusSdNbX100PgaEgaBG4DEPCWVpx19fXDSlnNh+8AzgM0H/pi8/lckokEurq6lJPT49JcME8RImJA0grNipIuVzOWphdUM9NY6jtV1RUlDQjuQaAewYA+W6Uayoe4C4Qrag+UCG5ceOG5ubm5Pu+ycTF43ENDg5a6vPiiy+q9+FQ0ampKY2PjyuZTBrw+/bbb/9QJgi93/X3RuBvWRsbG3rllVe0tLSkJ554wjbO3bt3Jcnm2A0ODuqf/tN/qi996Uu6du2aTp8+reHhYU1OTqqpqUlLS0u6f/++wuGwOjqKamscSA4ygBk1cQ4ITTvw1dnQbsccG5wDwPdd6SxKbi6GABhIhAIzcGtrSxMTEwaq9fX1KRwOW9gN2y+dTpsgKSrKHExanznkLvhHZEGKwRRiDBtGiopGoVAo0TB0OQCQlGABYgC5P+ASRAwwGJmaTATEay4tLWl4eNikwpGIm5ub06FDh5ROp9Xb26vf/u3fVn9/v9bW1nTz5k3dvHnTMJalpSXdunVLk5OTP5J9+9+7/t4I/B0rn8/r5s2bqqqq0vnz50v6wKEcl5cXx0z/+q//ul5++WXduHFDR44c0WuvvWabaHV1Vbdu3TLdP8JpQCyXxIInZMOTuxNuoz7j5rIYEcZ0uyrG0l7u7ApmUPtnKhB1c4ZloneHfgJelCghEoloc3NTXV1dikaj1gZcXl5uh5bSIOCeO8UZoA5vzMRmREqbm5vl+76VORlHhjEC88hms0b0ASh0QUUME+999+5dJZNJnThxwiISqWgY33rrLaXTaWMLhkIhbWxsqKOjw0hPv/iLv2i8gIqKCsMRCoXiTMkHDx5oeHj4xxYD2L/e69yBP5E0+PBHmiSlfd8/4xVViYcljTz83pu+73/+oC/6h702Nzf11ltvlcylO3z4sMbGxlRTU6P+/n7V1taqp6dHX/ziF/XHf/zH+vrXv26TgznoiURCb731ls6fP6/29naTr0KAgzHhkkoMg0t44Y9UBMcwDIT0lLo2NzeVy+VMww8JLwaduGIalOZisZjpK4bDYR06dEgjIyMWgSCttZ9RhxR3oVAo0WugdOeSfCgtwmWQZFEBKkMQiqARww/Y2toygRVSItIevLB7oAFyiRowCHV1dZqdnZXneerq6jKFYNKbiooKmxe4tLSkSCRiLNHf/M3f1MDAgO2L4eFhkwnb2trS9PS0rl279iORCXuv6z3NHfB9/0X+7Xnev5XkNkOP+b5/5qAu8MdlbW1t6c0339Ta2pqeeOIJVVRUKBqNmlw1XqW+vl4vvfSSOjs79fLLL2twcFCjo6NWn19cXNTrr7+uwcFB9fX1meLvzMyMNjY21NXVZX0CeFJoupB4ACUJ+ekMhKuPpySqcNuH8Z5ueU7aOzTw+untJ4Tmd4lgEOOQZKE3giQ0MFFexFC42oCUHWlM4lpdRiSTiKg8cMjplKQCAfo/Pz9fMjW5trZWjY2NZqR4fcg9KEehZeD7Rf3/lpYWaxQaGhrSzs6Oent79c/+2T/T0aNHDTu5du2abt++bQDt5OSkLl26ZCK1H5T1vuYOeEWX8IuSfupgL+vHc62vr+v69evK5XK6cOGCdnd3TS32zp076u/vN0/13HPP6U//9E+1urqqEydO6O7du8rn85ZnX7t2TaOjo2ppaTEmIcQTDqfnFRVwadelScitdbvTkRDrIL8mXcAT44WlvT4ESSWNOHADXHouRB1Cd3j88XjcWmfJ4dEZoGGHaIXr57Xc6IbfBeBMp9M27m11dVWbm5vGbnSVgDFSmUzGxnwjlwbph+t3pwPt7OwYX4F7UFtbq0OHDikSiWhiYkLl5eU6dOiQJiYm1Nvbqy984Qs6evSoPM9TJpPR5cuXrZU8mUzq0qVL+s53vvNjRQf+Qdf7xQSelrTg+/5952t9nuddk5SV9Du+73/nfb7Hj9Xa3i4OPMVTdnR0qKOjQ9lsVmNjY+rr67Ouul/7tV/Tb/3Wb2lxcVGnT5/WnTt3LD8Gyc9ms9afwPQc6tmQajgk5M3w8DEGkkpGZCPagRAIcmPgGW6PAobEFSkhfCakpdSInBZeNZfLaXV1VW1tbSVApSTL812kn2jEredzL8jboT7zfQBIUgGuz/d9k0xHxq2hoUG1tbVqbW01UJI+iZWVFSMDLS8vW3NUXV2d+vv7NTg4qPb2dv3RH/2RQqGQIpGIxsbGdPbsWb300ksaHBw0Y0NkNDMzo+XlZV29elW3bt36QKUA7nq/RuAfSvpj5/9xSd2+7y97nnde0p96nnfc9/13xEfeD2H4yKNasMLS6bQ+/vGPlzSs7Ozs6PDhwwqHw1pbW1N7e7v6+vqUTCbV1tZmnYHk6mNjY8pkMqb/v7y8rMHBQTU2Npo3lmQIOiUyUgLQevdwLS4uam1tTa2treadEboAoIM/4AqBuoq/eGhISK7Wfu7/b+9MY9u8zj3/OyIpSpQoSqJIidRmSdZmK3ZkW1bsOE4ap/GWpZO4QeKmWVCkQNEOpgUKNHfuh7mYfpkZdC7QwQx6p4Ok02mam962ztYsde24jdN6iW3JluxIlhftlERqsTZKtKR3PpDPCaXKiZOmpmS/f4AgdfSSfMiX53nPeZb/f2ICq9VKVlaWTsXJ54ePO/Rkqb6wKlBuEpSM5yiIJ+0Qx9Lb20tTUxNut5usrCwdM4hEIgwODmK321mxYoUOHAr/QCQS0VwNIjyysGRXKvoGBgaw2WyUl5drR3Hffffx3HPPsWLFCt0XMTQ0xPDwMMPDwzQ3N3P06NFlkQb8JHxuJ6CUsgKPAOtlzIjKj03HHp9USl0EKoiqFM2DcYPER/5eMAyDwcFB9u/fT19fH3V1dRQVFZGWlqbLTNesWcOPfvQjPdl/+MMfMjMTVawZHR3F6/ViGAZnz57VjTwXLlzQtOd5eXm43W5diy97ZSm4kSuvBAtTUlIYHBzU1FYejwdAF9UI05DkxK3WqNy43W5neHhYr0pkDy3KwsL8I8t4QK8UZHUidgiJp6T54kt7pV9Aoupy1Y/v6osnDZXjpEtR6MviBUH8fj/p6elaJEW+F5GMl+2UOABZ7cQXFckqo6ioiMuXL1NdXc33v/99KisrcTqdDA0N8d577zEzM4PH4+H8+fP8/ve/1wIiyx3XVTEYiwn8TrIDsbEdwD8YhnF33JgHGDIMY1YpVQocBm4zDGPoU15/2TmBeFgsFioqKti0aROrV6/GHtELIwAAHABJREFU6/WSlpbGmjVr5lXetba28pOf/ERrHwqpiEzmwcHBeYyzkroTwpHs7GxdtSettHJ1HR8fp6enhytXrrBy5UpKS0v1lVyukPGCIkpFmYv8fj9zc3P09/czOjqqA5JSmhwvLQZoSbKhoSG6urqAKC+f1+ud16EH6MDczMzMPBkv2Z4IJ0I8pbdsCebm5hgbG2NwcJDh4WEikYgma12sSEhqE6RWQPoGAoGAplaXtKPX69XqvxaLhdWrV5OTk0Nrayvbt2/nkUceoaqqCojKlTU0NDA0FP0Jnzlzhvfee2+e5PgywqIVg5/qBFSc7gDQD/wnwzBeUEr9X6IpwH+JO/ZR4D8T1RyYjR375qdZttydAESvZjk5OWzYsIENGzZQUFCAzWajtrZWF6ZAdKK/8MILvPPOO2RmZuJwOHQ13ezsLN3d3ZprMP7cSJlsWlqaVrgVFqPBwUGCwSCRSISSkhLq6uo0p15+fj5zc3MMDQ1pHkOpRPR6vVpWvb+/n8HBQc3AKxRfMvGEhBTQS2wJAMaXCkukX2oSpFVaYhdypV7YtyA1EfEly7J6kOCnbCFk2xX/XHktaY6SCr6FV2qpF5DVjcfj0XwG999/Pz/4wQ+wWCyMjIzQ1tbG2NgYdrudUCjEu+++y/vvv58wfYAvAJ/PCdwI3AxOQGC326mqqmLTpk1axFSizlKxFw6HaWpqYt++fRw7dgy3201eXh49PT309PQwOztLZWUlExMTtLS0aCILCSoCuupOgmESGJTeBqFNE9biYDCopa+kIEjINOfmokrCwp579epVampqyMnJ0QUygJbglhWDTFjh4JN8udfrxePx6EIpifpLM5EUM8mkj5/cUtIbv1KQuIMEOSXgKc5BVgXDw8P09PTQ3t6uU4WLISMjg6KiIsLhMOPj42RmZvLwww/z5JNP4nA4mJqaIhgM0tnZycjICK2trezfv1/Tii1jmE7gRiEpKYmcnBy+973vaUUdcQ6iUaCUIhQK8bvf/Y633nqLQCBAcXExVquVjo4OHnroIYqLizl06BChUIjR0VGdlYiHLK9lsmVmZlJZWUlhYaEuN5Y9tugNjI6OauakvLw8HA6HZhQSQQzREpStxOjoKP39/fPUgmR/Lct34f+bnJxkxYoVFBYWzqtjEIKSSCSiC69kAstyXuIR0qMgUXwJZMaTfkiQLxwOEwwGOX/+PH19fczMzGiiEqWU1mOUxiWxa3R0lK1bt7Jnzx6tXH3w4EFNeNLS0sKf/vQnzp07t2wj/wtgOoEbjfLycurr61m7dq3uRkxPT9eEona7XUew33rrLd58800GBwdxu906kzA0NMTo6CiXL1/W+1IpaZWUm9yLqlBJSQk1NTW43W4APcGkvj4cDjM2NkZqaiperxe3243D4Zi3UpBAIaB7HOIrFZVSOkgnSkQSN5DsgaRLhdxTKUVvby/T09Oazjw+NSjHSLWjVCRKTYLUOsT3UMzMzDA4OEggENCCrk6nk9raWrZt28a5c+d45513yMrKorCwUGdO6urqePzxx1m7dq0mfbVarTQ2NnLp0iUaGhr44IMPtMjoTQLTCSQCqampVFRUsG7dOlatWsWaNWt0hLysrEzvnQF6enr4zW9+w/79+xkeHua2227DZrPR0tLC5cuX9R44OTmZ0tJSfXWWwpjx8XEd2PP7/VRXV1NUVKQrD+Hj6r5AIMDc3Bwej0f3BggZplx9BwYGOHv2LNPT03i9XuBjrsP4SLuwE0l8QKLxQjwqBUORSIT+/n5mZ2cpKirC7XbrrYX8DuMZhKRqsK+vj87OTh1LkG2HfB5Je+bk5ODz+Vi5ciUZGRn09PTQ2NiI0+mkrKyMCxcuMDMzw5NPPsnXvvY10tPTCYfDnDp1ikgkgtPp5O233+bAgQP09PTo7+EmgukEEgm73c7KlSupq6tjzZo1VFVVYbfbaW9vp7S0lJUrV+oeAKlA+/Wvf017e7uW0Ja4gGwPDCNKpe3z+QiFQkQiETIzM3W6USmF3++noqICn8+n99LSEj02NkZBQQEej0en8qQOQTQBZOsgaTy5+kuhk0x42RLIxLFYLFqzUJ5jtVo1bZfX66WgoGDeGKBXFaJtGAqFtMbAwj2+OJfS0lLq6+u58847SUpK4sSJE+zfv5+BgQEKCgpwOBx0dHSwZcsWvv3tb1NRUaHbipubmwmFQhiGwV/+8hdeeeWVhGkC3gCYTmApQIpshI2msrKS/Px80tLSOHv2LC6XizvvvBOr1crg4CCHDx/mtddeo6Ojg6ysLHw+H5FIhKNHj+oGm/r6ej0Zy8rKcLlc9PT0cP78efr7+7HZbLok2e/3Mzs7qyvt8vLyKCsr04rHUt47OTmplYviG4iknl/updpwYTZDVgniPCS7ISsHyf+npaXNqw2Il3GXegCn08no6Kim+MrKyqKgoIDi4mIyMjI0zVd7ezutra36M2dkZBAMBikoKOC73/0ud999N3a7XWsRXr58ma6uLgKBAI2NjZw7d+5mdgBgOoGlA9lTS7FOTU0Nd911F9nZ2ZqVKDc3F0C3yko24c9//jMFBQVkZ2fT3NxMb2+vTuXZbDYqKirYuHEj4XCYAwcO6J5/WUILQxGg98FCfyZRemnNlYCaFAnJlgLQe3jhOxCdxXjVn4yMDE3SIaSfUqy08PsQpKamkpOToxuLpMDI7XZrfsYVK1Zoqq/jx49z8uRJXQuQmZlJTk6OVkj+yle+wt69e/F6vYTDYbq7u2lra9NZhFOnTtHW1nazT36B6QSWMjweD7W1tdTW1nLbbbdRVFSEz+fTar5SdXfx4kVeffVV/vjHP+qUmkxyqYrzeDwUFBTgdrvZsmULExMTvPvuu1rnYDGC0vhSXpnwsoSXdl6Hw6EpuVwul574IvEl/RR2u52hoSH6+/v1Xh3QTTuBQEBLlo+NjeFyufQ2QJSGpqamGBoa0hqPW7ZsoaqqShcCdXZ28tFHHxEMBklOTsbpdGobkpKS2LFjh+77n5qaYmBggO7ubkKhEK2trXz44Ye0tLQs55z/54HpBJYDJGKfm5vLl7/8ZbZu3UpSUhKjo6O6lDgSidDW1saRI0c4fPgwHR0duhNRSoOFjae8vJzy8nJqampIT0/nV7/6FV1dXTqrIBV7kkGQ3L3b7cbpdGqtAp/Pp0lQ8/PzcblcOqjn9/vp6+sjFApht9sZGRnR5JojIyPa4eTk5FBRUaGrHh0Oh9ZGlAzE4OCgLpYS+jVRUQoGg7pWQjItTqdTtzlnZ2dzxx138MADD1BRUaF5G0U16OLFixw4cEDbtRR++zcYphNYbnA4HOTm5rJ+/Xruuece1q5dS0pKitYonJmZYWRkhKamJk6fPs2xY8dob2/XkXupi5+dnaWgoID6+no9yWQSpaWl6WKljIwMRkZG6OnpIS0tjZycHM2xt3LlSh0slDZlmaybN2/WjThDQ0N0dnbqWMLg4CC9vb36WNFFjG9c6uzsxOl0YrFYCIVCurdByD8nJiY04Yp0Bkopdnp6OrfddhsbNmxg/fr1FBcXEw6HOXPmDBcuXMBqtRIMBjlx4gQtLS06CHiLwnQCyxWyvy4qKmLt2rVs27aNkpISXYMvzTTj4+M0NTXx5ptvcubMGZ3Xl61EfJTdMAy9p3e5XOTm5uqrqlCUCQkIoPfoY2NjzMzMkJaWpkuAJS0pqwC5MkvnngiEGMbHIqXxdOPS1x9vmwQUk5OTNc2YrACSk5MpLy9ny5Yt3H777ZSUlOjuzLGxMVpaWmhpaeHChQscP35csxzfRPn+zwvTCdwMsNlsZGdnU11dzZo1aygtLWXVqlV6kkpn4ujoKF1dXRw4cEBHzK9cuUJaWpqm2B4fH9fceNKdGJ+Gi+cCjKf4EokzqS2QajzZWkgloNQkSKZgdnZWtwEvVCcSpwAfZyikNNhqteqW7Pr6eurq6jT/ggQdA4EAFy9epKmpSQdMx8bGlnuZ7xcN0wncbJD9uhQhbd26ldWrV+urvtTfh8Nhent7aWtr46233mJiYoKBgYF5vIRCNyb6AsLKG88pKFdaUeUVolDRDYyXO5N0oigvSfzB5XJhs9n0slyciJQWi5hpSkoKVVVVFBcXc/XqVaqqqigrK9PNQxMTE7S1tdHe3k5zczOnTp3S1YDmxL8mTCdws0Ii8n6/n7Vr1+o++IqKCk0FLpV709PTWulIFIcGBga4dOmSFhnt7u7WoidCMWaz2cjPz8fn82mK8qmpKXJycsjNzdUko1arVWcBhMFH3luu+FJi7Ha7CYVCmpLL5XLh9/u10xBykenpacbHx+ns7KSrq4uuri7a2tq4ePEifX195sS/fphO4FaBdA96PB4cDgclJSX4fD6ysrKora0lPz8fv9+va/YlNSgIh8O68SgcDs+TPQPmpRdHR0fp6+sjJycHm82mRVXiG3iGhoY0wWdbWxtdXV3U1dUBUVo0iUUIGciZM2cIBAJ0d3fT3NysqxYnJycZHh7W25il8NtdZjCdwK0KuQpbLBby8vIoLi7G5/PpRiXZv6ekpFBRUUFJSYm+EqekpOjOxfHxcdrb23Wg0Waz6RZkr9dLcXExLpeLYDBIT0+PluY+evQoSUlJ5OXl6TJgp9PJ5OQkoVCI9PR0vQoJBoM6m2AG875wfG5SkUKidON5wBzwU8MwfqyUygZ+BawA2oHHDMMYVtHLxI+BXcAk8IxhGKc+5T1MJ5AASG2A0G0JjXpZWRk5OTnaWWRkZGjyk76+Pq0RGM/YI0U80uATCAQ0q4/0OQD6vYQVSSa5NAIthYvSTYzP7QR8gM8wjFNKKSdwEvgK8AxRKrH/opR6HsgyDOMHSqldwL8n6gTqgR8bhlH/Ke9hnvklBClpFoYgYTeO1z2QAKBM3IX8gAt7CUwsCSzqBPTJut4b8DrwZaIqQ77YmA9ojT3+38ATccfr4z7hNQ3zZt7M29/9dmKx+Rclhb9OxAhHa4FjQK5hGAGA2L03dlg+0BX3tO7YmAkTJpYgrptyXCmVDvwW+K5hGKPxnV8LD11kzFjk9Zat7oAJEzcTrmsloJSyEXUAvzQMY19suD8WL5C4wUBsvBsojHt6AdC78DUNw/ipYRgbFt2jmDBh4obhU51ALNr/AvCRYRj/HPevN4CnY4+fJhorkPGnVBR3AFdk22DChImlh+vJDmwhKiLSRDRFCPAficYF/g0oAjqBrxqGMRRzGv8T2EE0RfisYRh/pUC04D0+2QgTJkx8ETCLhUyYuMWxqBP4TNkBEyZMfHGwWq04nc5F/2ez2SgpKdE6j9cDUYCOxycE8D9+3nW/gwkTSxTX80O/3uNFC+FaxwndmsBiseD1eueNXS98Ph979+7V75WZman/53K5+OpXv0pOTs6ikzv+swjHpNgt3ZiiXflpMJ2AiRsC6V9YCPnh/i1YuKWV90lOTiY7O1uPFRQUkJeXh2EYFBcXU1pa+lev9cQTT2iNhcVet76+nu3bt+sxh8PBN77xDQoLC/UxNptNl2MLhAfR5XLpcaGAAygsLOTZZ59l3bp1+P1+3d7t8XjYtm2b7sSMh8Vioa6ujqeffpovfelLbN68GYCNGzeydetW6uvr2bt37yc6EfgbpMlNmICP+w+kqehaLb0yodxuN+FwWMuZ7dixg0OHDs3rCozvKfgscLlcOJ1OLUW2cuVKamtrefnll1m1ahUbNmzgt7/9LRDtXoyfHMKwJJJuECV/LSkpISUlhZMnTzIxMcHc3JxWf5JJKXwKSUlJ1NTUUFlZidVqpbOzkyNHjpCamsq2bduYnp5mxYoVdHR0cPDgQc2UJA1cWVlZzM7OEg6HNS372rVrmZiY4N5772Xfvn2ahMUwDHJzc7n//vt56aWX6Orq0t9fQ0ODVpVav349KSkp+vteDKYTMPG5kJKSQm1t7TzF4YmJCZqamrR68UIUFhbyne98h4aGBvbt20ckEqGoqAin06mVhScnJ/WPWa7kNptNi50opSgsLNSNSR6PB7fbTVtbG7t376ayspLTp09z6NAhxsfHsVqtlJeXc++99/LSSy9p7gJplHr11VfJzs5mz549dHd3k5WVpZfXTzzxBI2NjXR0dGinNDMzQ1VVFY899hiA5l1ISkrC6/Wyc+dOXnvtNZKSkrj//vsJBAJcunSJgwcPkpWVxdzcHDU1NRw+fFhTtKekpNDd3a2p0YRPMSkpiQ8//BCXy8Xq1av19yjfj8PhYHp6mtHRUX0ObDYbe/bsIRgM0tjYyPT0NOnp6Z/oBMztgInPDLvdzoMPPkhHRweBQICpqSktFfbQQw8tGuxKTk6mvr6eDz/8EL/fT1lZGRDVPrj77rt59NFHefbZZ3nmmWc0fdk3v/lNNm7cSElJCU8//TT5+fnYbDY2b96sn+/xeNi0aROzs7O0trYSCAQ4dOiQnhjZ2dk8+uijHDp0iJGREQDNlmS327FarWzcuJGRkRFNy56Wlsb09DSNjY2sX7+e+vp6MjMzNQtSR0cHb7/9Nq+//jr79u3TAqii65CWlkZSUhJvv/02wWAQj8fD17/+dTweD6FQaJ6oqjwXog5G1KSFsi05OZmxsTHNzeB2u9m5cydOp5Pe3l4GBgbYuXMnxcXFVFRUaBFal8tFVVUVMzMzbN26VYvLLAZzJWDiM6OoqIizZ89SXV3N3NwcH3zwgd4KHD9+nNtvv53Dhw/Pe47H46GiooIXX3yRqqoq7rrrLrq7uzVr0IEDB7DZbDz//PNUVVXhcrkIhUK88cYb+jUeeOABXnjhBa5evaqX3yKgopRiYmJCy5/JlXF8fJz9+/eza9cuMjIyaGho0IKrsvyenZ3VbEZGTNptbm6O48ePc/r0aWpra3nwwQd55ZVXCIfDersiPAuTk5NYrVaGhoZoaWmhvLycY8eOzXM2aWlpWi1JKUVxcTGXLl3SKlJKKY4cOcJjjz3G0aNHOXnyJBcuXGBsbIyxsTFOnz6tqdWEdn1ycpKXX36Z/Px87WDGx8f55S9/ic/nIxwOc/z4cTIzMz9RVdl0AiY+M2T/Pzc3p1uNhQ9AlqgyoSAaLFu9ejWRSIT169eTmppKXl4e69evZ3p6WkuZWSwWRkZGsFgsTE9P62X51NSU5j2cmZnhypUrOJ1OUlNTKSws1HRnsvzPy8vTDMOihPziiy+ya9cu/H4/77zzjiZBBThy5AhPPfUUDz74IFarVb/3unXryM/PJyUlhVOnTjE+Po7D4aC1tXWeYtHrr7/OwMAAV69e1ZPS5/MxNDREOBxmdHSUn/3sZ2RmZjIwMKBJVsbGxnjppZeIRCIYhsHRo0c5ceKEdqg///nP9XcYDAY1k9OhQ4fmnYv29nba29v1mNC4Cz5pKwBmsZCJz4GMjAxqa2vp7OzE5/Npfb/h4WHWrFnDe++9p0VFIBo/+Na3vsUvfvELxsfHMQyDsrIyduzYwblz56ipqeH999/XUmNvvvkmV69eZffu3dpJuN1u/vCHPxAKhfD5fOzevZtwOExzczM+n493330Xu91OdXU1fr+fU6dOMTQ0pNWIZfXg8/kIBAJcvXqVjIwMLffucDjIysoiGAxqB5GUlDRv2X4TwKwYNPHFITs7m9LSUi0gopTS+3LZewtyc3MpLCzkxImPq8etVisrVqxgeHiYlJQUTUs+MDCgl9kiWirkpfFX3/jVx0J83uzCLQDTCZj4YiFCIDabjZmZGaanp83Jt7SxqBMwYwImPjckOCZXbhPLE2aK0ISJWxymEzBh4haH6QRMmLjJca2+DYEZEzBh4gYhXrkpKSlpXh2FBFQtFgsOh4PJyUmt+SgqUKLx6HQ6SUlJIRgMAtF+jIqKCqxWKx0dHXR3d1NdXU1paSnt7e0UFxfT2dnJmTNnFrXLXAmYMBFDvFKTICUlRQukyiROTU0lOTl53nPz8vL08zMyMrj99tvZvHkzNTU12Gw2XC4X27dv55577qG6uprdu3fj9XopKSnhzjvvxGq1kpyczCOPPML27dvJzs4mOTmZ5557DovFwrZt26itrSUpKYmKigrWrVuHUorc3FweffRRrFYrly9fJhwOU1BQwH333cfp06exWq3cdddd9PX1XfNzm07AxLKFCKBaLBas1o8Xtenp6Xg8HiBa2JSZmaknsM/no6ysDIvFwqpVq/D7/fq4hx56iE2bNvHII4+QlZVFRkYGW7du5Z577mHPnj1UVVUB0c7HLVu2zOtCfOqpp3A4HDgcDvbu3YvD4aC/v5+xsTEAVq9ejd/vp6GhgfLyctxuN8FgkMzMTCorK5mdnSUrK4vKykref/99QqEQkUiE5ORkZmdn5yk0NTQ0cPDgQSBawm232/nggw/o7u7WUm7Nzc3U1NTg9XqZmpr6xNTtUqkTCAITQCjRtvwNyGF52w/L/zMsd/vh7/sZig3D8CwcXBJOAEApdWI5048vd/th+X+G5W4/JOYzmNsBEyZucZhOwISJWxxLyQn8NNEG/I1Y7vbD8v8My91+SMBnWDIxARMmTCQGS2klYMKEiQQg4U5AKbVDKdWqlLqglHo+0fZcL5RS7UqpJqVUo1LqRGwsWyn1B6VUW+w+K9F2xkMp9aJSakAp1Rw3tqjNMS3J/xE7L2eUUusSZ7m2dTH7/0kp1RM7D41KqV1x//uHmP2tSqnti7/qjYNSqlApdUgp9ZFS6qxS6j/ExhN7DgzDSNgNsAAXgVIgGTgNrEqkTZ/B9nYgZ8HYfwOejz1+HvivibZzgX1bgXVA86fZDOwC3iEqNX8HcGyJ2v9PwPcXOXZV7PdkB0pivzNLgu33Aetij53A+ZidCT0HiV4JbAQuGIZxyTCMCPAK8HCCbfpb8DDw89jjnwNfSaAtfwXDMN4HhhYMX8vmh4H/Z0RxFMgUKfpE4Rr2XwsPA68YhjFtGMZl4ALR31vCYBhGwDCMU7HHY8BHQD4JPgeJdgL5QFfc392xseUAA9ivlDqplPpmbCzXiMmwx+69CbPu+nEtm5fTuflObLn8YtwWbEnbr5RaAdQSVfdO6DlItBNYrL9xuaQr7jQMYx2wE/i2Umprog36grFczs1PgDLgdiAA/PfY+JK1XymVDvwW+K5hGKOfdOgiY1/4Z0i0E+gGCuP+LgB6E2TLZ4JhGL2x+wHgVaJLzX5ZrsXuBxJn4XXjWjYvi3NjGEa/YRizhmHMAf+Hj5f8S9J+pZSNqAP4pWEY+2LDCT0HiXYCHwLlSqkSpVQy8Djwxqc8J+FQSqUppZzyGLgfaCZq+9Oxw54GXk+MhZ8J17L5DeCpWIT6DuCKLFmXEhbskf8d0fMAUfsfV0rZlVIlQDlw/EbbFw8VbWV8AfjIMIx/jvtXYs9BIqOlcRHQ80Sjt/+YaHuu0+ZSopHn08BZsRtwAweBtth9dqJtXWD3vxJdMl8lepX5xrVsJroU/V+x89IEbFii9v8iZt+Z2KTxxR3/jzH7W4GdS8D+LUSX82eAxthtV6LPgVkxaMLELY5EbwdMmDCRYJhOwISJWxymEzBh4haH6QRMmLjFYToBEyZucZhOwISJWxymEzBh4haH6QRMmLjF8f8BI4xAbobnUGMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DATASET(SHAPE, 1, range(4), BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=True)\n",
    "\n",
    "for ix, data in enumerate(dataset.data_generator()):\n",
    "    img, y = data\n",
    "    print(img)\n",
    "    print(img.shape)\n",
    "    print(\"-\"*10)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"-\"*10)\n",
    "    print(img[0,:,:,:].shape)\n",
    "    plt.imshow(img[0,:,:,:])\n",
    "    plt.show()\n",
    "    \n",
    "    if ix==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NZygcwwp0Sry"
   },
   "outputs": [],
   "source": [
    "# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "    \n",
    "    Only computes a batch-wise average of recall.\n",
    "    \n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    \n",
    "    Only computes a batch-wise average of precision.\n",
    "    \n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisionx = precision(y_true, y_pred)\n",
    "    recallx = recall(y_true, y_pred)\n",
    "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bBbVuHAfgHnh"
   },
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IvvEuAAKfkV5"
   },
   "outputs": [],
   "source": [
    "# copied from https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "    \n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature._keras_shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature._keras_shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool._keras_shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool._keras_shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat._keras_shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature._keras_shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F4Ndx2vm6NZ3"
   },
   "outputs": [],
   "source": [
    "# copied from https://gist.github.com/mjdietzx/5319e42637ed7ef095d430cb5c5e8c64\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    shortcut = y\n",
    "\n",
    "    # down-sampling is performed with a stride of 2\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    y = add([shortcut, y])\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EVUWz9lzfm6Y"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    dropRate = 0.3\n",
    "    \n",
    "    init = Input(SHAPE)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
    "    \n",
    "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(2, activation='softmax')(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3001
    },
    "colab_type": "code",
    "id": "w2V3AUW7fm-n",
    "outputId": "d7c45fcc-739a-4166-9a30-d65352088c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 224, 224, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 224, 224, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 224, 224, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 224, 224, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 224, 224, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 112, 112, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 112, 112, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 64)     0           global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 1, 8)      520         reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 1, 64)     576         dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 1, 64)     0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 1, 64)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 112, 112, 64) 0           activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 112, 112, 2)  0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 112, 112, 1)  98          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 112, 112, 64) 0           multiply_1[0][0]                 \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 112, 112, 64) 36928       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 112, 112, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 112, 112, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 112, 112, 64) 36928       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 112, 112, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 112, 112, 64) 0           multiply_2[0][0]                 \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 64) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 56, 56, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 128)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 128)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 128)    0           global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 1, 16)     2064        reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 1, 128)    2176        dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 1, 128)    0           dense_4[0][0]                    \n",
      "                                                                 dense_4[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1, 1, 128)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 56, 56, 128)  0           activation_5[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 56, 56, 2)    0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 56, 56, 1)    98          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 56, 56, 128)  0           multiply_3[0][0]                 \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 56, 56, 128)  147584      multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 56, 56, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 56, 56, 128)  147584      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 56, 56, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 56, 56, 128)  0           multiply_4[0][0]                 \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 224, 224, 32) 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 224, 224, 64) 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 128 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 224, 224, 224 0           up_sampling2d_1[0][0]            \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 224)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          57600       global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256)          1024        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 256)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256)          1024        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            514         activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 605,566\n",
      "Trainable params: 603,262\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "schfFpcIfzZy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5584
    },
    "colab_type": "code",
    "id": "MSHMA1gtMQ6e",
    "outputId": "61dadec3-f4b4-4877-970a-c66700175ab5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 8s - loss: 0.7432 - precision: 0.6583 - recall: 0.6583 - f1: 0.6583 - acc: 0.6583 - val_loss: 9.1712 - val_precision: 0.3750 - val_recall: 0.3750 - val_f1: 0.3750 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.17123, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.5459 - precision: 0.7667 - recall: 0.7667 - f1: 0.7667 - acc: 0.7667 - val_loss: 6.2091 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00002: val_loss improved from 9.17123 to 6.20908, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.5083 - precision: 0.7667 - recall: 0.7667 - f1: 0.7667 - acc: 0.7667 - val_loss: 5.4912 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.20908 to 5.49120, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.4525 - precision: 0.8083 - recall: 0.8083 - f1: 0.8083 - acc: 0.8083 - val_loss: 3.7202 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.49120 to 3.72016, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.5095 - precision: 0.7000 - recall: 0.7000 - f1: 0.7000 - acc: 0.7000 - val_loss: 3.4042 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.72016 to 3.40421, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 6/100\n",
      " - 3s - loss: 0.5147 - precision: 0.7667 - recall: 0.7667 - f1: 0.7667 - acc: 0.7667 - val_loss: 0.8084 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.40421 to 0.80836, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 7/100\n",
      " - 3s - loss: 0.4038 - precision: 0.7833 - recall: 0.7833 - f1: 0.7833 - acc: 0.7833 - val_loss: 2.9552 - val_precision: 0.4167 - val_recall: 0.4167 - val_f1: 0.4167 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.80836\n",
      "Epoch 8/100\n",
      " - 3s - loss: 0.3879 - precision: 0.8250 - recall: 0.8250 - f1: 0.8250 - acc: 0.8250 - val_loss: 1.5984 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.80836\n",
      "Epoch 9/100\n",
      " - 3s - loss: 0.3813 - precision: 0.8083 - recall: 0.8083 - f1: 0.8083 - acc: 0.8083 - val_loss: 0.9431 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.80836\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.3186 - precision: 0.8583 - recall: 0.8583 - f1: 0.8583 - acc: 0.8583 - val_loss: 1.0173 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.80836\n",
      "Epoch 11/100\n",
      " - 3s - loss: 0.3590 - precision: 0.8583 - recall: 0.8583 - f1: 0.8583 - acc: 0.8583 - val_loss: 2.8673 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.80836\n",
      "Epoch 12/100\n",
      " - 3s - loss: 0.3615 - precision: 0.8333 - recall: 0.8333 - f1: 0.8333 - acc: 0.8333 - val_loss: 1.6537 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.80836\n",
      "Epoch 13/100\n",
      " - 3s - loss: 0.3348 - precision: 0.8500 - recall: 0.8500 - f1: 0.8500 - acc: 0.8500 - val_loss: 1.1280 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.80836\n",
      "Epoch 14/100\n",
      " - 3s - loss: 0.2846 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 2.7183 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.80836\n",
      "Epoch 15/100\n",
      " - 3s - loss: 0.2469 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 0.6230 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.80836 to 0.62299, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 16/100\n",
      " - 3s - loss: 0.3305 - precision: 0.8667 - recall: 0.8667 - f1: 0.8667 - acc: 0.8667 - val_loss: 2.2573 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.62299\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.3181 - precision: 0.8917 - recall: 0.8917 - f1: 0.8917 - acc: 0.8917 - val_loss: 6.1006 - val_precision: 0.3750 - val_recall: 0.3750 - val_f1: 0.3750 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.62299\n",
      "Epoch 18/100\n",
      " - 3s - loss: 0.3303 - precision: 0.8250 - recall: 0.8250 - f1: 0.8250 - acc: 0.8250 - val_loss: 1.2510 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.62299\n",
      "Epoch 19/100\n",
      " - 3s - loss: 0.2873 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 1.5083 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.62299\n",
      "Epoch 20/100\n",
      " - 3s - loss: 0.2886 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 1.5335 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.62299\n",
      "Epoch 21/100\n",
      " - 3s - loss: 0.3784 - precision: 0.8417 - recall: 0.8417 - f1: 0.8417 - acc: 0.8417 - val_loss: 1.3547 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.62299\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.2314 - precision: 0.8917 - recall: 0.8917 - f1: 0.8917 - acc: 0.8917 - val_loss: 0.8811 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.62299\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.2162 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.8896 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.62299\n",
      "Epoch 24/100\n",
      " - 4s - loss: 0.2073 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.9549 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.62299\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.2662 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 0.6187 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.62299 to 0.61875, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 26/100\n",
      " - 3s - loss: 0.2590 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 0.1965 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.61875 to 0.19652, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 27/100\n",
      " - 3s - loss: 0.1939 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.9545 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.19652\n",
      "Epoch 28/100\n",
      " - 3s - loss: 0.2014 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.7293 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.19652\n",
      "Epoch 29/100\n",
      " - 3s - loss: 0.1980 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 0.1972 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.19652\n",
      "Epoch 30/100\n",
      " - 3s - loss: 0.1393 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.4509 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.19652\n",
      "Epoch 31/100\n",
      " - 3s - loss: 0.2332 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 1.0525 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.19652\n",
      "Epoch 32/100\n",
      " - 3s - loss: 0.3211 - precision: 0.8917 - recall: 0.8917 - f1: 0.8917 - acc: 0.8917 - val_loss: 1.2618 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: val_loss did not improve from 0.19652\n",
      "Epoch 33/100\n",
      " - 3s - loss: 0.3072 - precision: 0.8917 - recall: 0.8917 - f1: 0.8917 - acc: 0.8917 - val_loss: 1.2978 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.19652\n",
      "Epoch 34/100\n",
      " - 3s - loss: 0.2186 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.0853 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.19652\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.2952 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 1.5943 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.19652\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.2136 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 0.9522 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.19652\n",
      "Epoch 37/100\n",
      " - 3s - loss: 0.2218 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 0.8907 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.19652\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.1754 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.1549 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.19652\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.1680 - precision: 0.9000 - recall: 0.9000 - f1: 0.9000 - acc: 0.9000 - val_loss: 1.1966 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.19652\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.1732 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 1.5194 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.19652\n",
      "Epoch 41/100\n",
      " - 3s - loss: 0.2086 - precision: 0.9000 - recall: 0.9000 - f1: 0.9000 - acc: 0.9000 - val_loss: 0.9610 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.19652\n",
      "Epoch 42/100\n",
      " - 3s - loss: 0.2389 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 1.0141 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.19652\n",
      "Epoch 43/100\n",
      " - 3s - loss: 0.1965 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 2.3347 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.19652\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.1856 - precision: 0.9000 - recall: 0.9000 - f1: 0.9000 - acc: 0.9000 - val_loss: 0.7297 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.19652\n",
      "Epoch 45/100\n",
      " - 3s - loss: 0.3510 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 1.6616 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.19652\n",
      "Epoch 46/100\n",
      " - 3s - loss: 0.1127 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 2.4110 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.19652\n",
      "Epoch 47/100\n",
      " - 3s - loss: 0.1171 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 1.5272 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.19652\n",
      "Epoch 48/100\n",
      " - 3s - loss: 0.1546 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.7132 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.19652\n",
      "Epoch 49/100\n",
      " - 2s - loss: 0.0906 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 1.4371 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.19652\n",
      "Epoch 50/100\n",
      " - 3s - loss: 0.1328 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.8179 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.19652\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.1327 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.1491 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.19652\n",
      "Epoch 52/100\n",
      " - 3s - loss: 0.0917 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.6342 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.19652\n",
      "Epoch 53/100\n",
      " - 3s - loss: 0.0751 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.7053 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.19652\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.0657 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 1.5356 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.19652\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.2133 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 1.4219 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.19652\n",
      "Epoch 56/100\n",
      " - 3s - loss: 0.0789 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 1.0994 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.19652\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.0801 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.5715 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.19652\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.0963 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.4376 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.19652\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.0688 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.2258 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.19652\n",
      "Epoch 60/100\n",
      " - 3s - loss: 0.0995 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 2.2321 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.19652\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.0815 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.9619 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.19652\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.0703 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.5422 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.19652\n",
      "Epoch 63/100\n",
      " - 3s - loss: 0.0308 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.4988 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.19652\n",
      "Epoch 64/100\n",
      " - 2s - loss: 0.0716 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.9307 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.19652\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.0625 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 1.1635 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: val_loss did not improve from 0.19652\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.0565 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.3916 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.19652\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.0342 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 1.0415 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19652\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.0792 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.4040 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19652\n",
      "Epoch 69/100\n",
      " - 4s - loss: 0.0241 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 1.9247 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19652\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.0663 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.8183 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19652\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.0621 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.7809 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19652\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.0743 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 1.5357 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19652\n",
      "Epoch 73/100\n",
      " - 3s - loss: 0.0554 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 1.5630 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19652\n",
      "Epoch 74/100\n",
      " - 3s - loss: 0.0591 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 1.3901 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.19652\n",
      "Epoch 75/100\n",
      " - 3s - loss: 0.1575 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.0679 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19652\n",
      "Epoch 76/100\n",
      " - 3s - loss: 0.2403 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 0.5144 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19652\n",
      "Epoch 77/100\n",
      " - 3s - loss: 0.1503 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 2.0605 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19652\n",
      "Epoch 78/100\n",
      " - 3s - loss: 0.1331 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.9304 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.19652\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.3555 - precision: 0.8750 - recall: 0.8750 - f1: 0.8750 - acc: 0.8750 - val_loss: 0.4396 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.19652\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.1492 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.8064 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.19652\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.0899 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.8251 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.19652\n",
      "Epoch 82/100\n",
      " - 3s - loss: 0.2259 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 2.7259 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19652\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.1717 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 3.5868 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.19652\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.1245 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 4.3326 - val_precision: 0.3333 - val_recall: 0.3333 - val_f1: 0.3333 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.19652\n",
      "Epoch 85/100\n",
      " - 3s - loss: 0.2121 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 4.0756 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.19652\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.1036 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 1.3256 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.19652\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.2274 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.4522 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.19652\n",
      "Epoch 88/100\n",
      " - 3s - loss: 0.1526 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.8670 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.19652\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.1220 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.0594 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.19652\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.1517 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.4720 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.19652\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.1427 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.4006 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.19652\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.1005 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.8326 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.19652\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.1576 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.5097 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.19652\n",
      "Epoch 94/100\n",
      " - 3s - loss: 0.1221 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.6000 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.19652\n",
      "Epoch 95/100\n",
      " - 3s - loss: 0.0941 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0784 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.19652 to 0.07842, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.2187 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 1.5118 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.07842\n",
      "Epoch 97/100\n",
      " - 3s - loss: 0.1067 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.5507 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.07842\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.1926 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.5001 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00098: val_loss did not improve from 0.07842\n",
      "Epoch 99/100\n",
      " - 3s - loss: 0.1141 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 2.0454 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.07842\n",
      "Epoch 100/100\n",
      " - 3s - loss: 0.0720 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 1.9006 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.07842\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.4135 - precision: 0.8333 - recall: 0.8333 - f1: 0.8333 - acc: 0.8333 - val_loss: 0.0465 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04648, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.5094 - precision: 0.8167 - recall: 0.8167 - f1: 0.8167 - acc: 0.8167 - val_loss: 2.9635 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04648\n",
      "Epoch 3/100\n",
      " - 3s - loss: 0.4478 - precision: 0.8250 - recall: 0.8250 - f1: 0.8250 - acc: 0.8250 - val_loss: 0.6497 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04648\n",
      "Epoch 4/100\n",
      " - 3s - loss: 0.2930 - precision: 0.9000 - recall: 0.9000 - f1: 0.9000 - acc: 0.9000 - val_loss: 1.2487 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04648\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.2946 - precision: 0.8667 - recall: 0.8667 - f1: 0.8667 - acc: 0.8667 - val_loss: 2.8859 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04648\n",
      "Epoch 6/100\n",
      " - 3s - loss: 0.1668 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 2.0031 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04648\n",
      "Epoch 7/100\n",
      " - 3s - loss: 0.2112 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.4568 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04648\n",
      "Epoch 8/100\n",
      " - 3s - loss: 0.1658 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.1944 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04648\n",
      "Epoch 9/100\n",
      " - 3s - loss: 0.1763 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.9240 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04648\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.2137 - precision: 0.9000 - recall: 0.9000 - f1: 0.9000 - acc: 0.9000 - val_loss: 0.6227 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04648\n",
      "Epoch 11/100\n",
      " - 3s - loss: 0.1616 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.6545 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.04648\n",
      "Epoch 12/100\n",
      " - 3s - loss: 0.2339 - precision: 0.8917 - recall: 0.8917 - f1: 0.8917 - acc: 0.8917 - val_loss: 0.5914 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04648\n",
      "Epoch 13/100\n",
      " - 3s - loss: 0.1950 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.4897 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04648\n",
      "Epoch 14/100\n",
      " - 3s - loss: 0.1584 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.7516 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04648\n",
      "Epoch 15/100\n",
      " - 3s - loss: 0.1707 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 1.2072 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04648\n",
      "Epoch 16/100\n",
      " - 4s - loss: 0.1786 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.2648 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04648\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.1117 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.6792 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04648\n",
      "Epoch 18/100\n",
      " - 3s - loss: 0.2703 - precision: 0.9000 - recall: 0.9000 - f1: 0.9000 - acc: 0.9000 - val_loss: 1.6379 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04648\n",
      "Epoch 19/100\n",
      " - 3s - loss: 0.1719 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 2.4857 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04648\n",
      "Epoch 20/100\n",
      " - 3s - loss: 0.1261 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.3870 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04648\n",
      "Epoch 21/100\n",
      " - 3s - loss: 0.1200 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.2443 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.04648\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.1497 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1041 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.04648\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.1552 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.4741 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.04648\n",
      "Epoch 24/100\n",
      " - 3s - loss: 0.3052 - precision: 0.8667 - recall: 0.8667 - f1: 0.8667 - acc: 0.8667 - val_loss: 0.4453 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.04648\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.0803 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.1796 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.04648\n",
      "Epoch 26/100\n",
      " - 3s - loss: 0.1086 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.1138 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.04648\n",
      "Epoch 27/100\n",
      " - 3s - loss: 0.1769 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.6714 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.04648\n",
      "Epoch 28/100\n",
      " - 3s - loss: 0.0901 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.2847 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.04648\n",
      "Epoch 29/100\n",
      " - 3s - loss: 0.1828 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.3040 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04648\n",
      "Epoch 30/100\n",
      " - 3s - loss: 0.1096 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.4461 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.04648\n",
      "Epoch 31/100\n",
      " - 3s - loss: 0.0778 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.8685 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss did not improve from 0.04648\n",
      "Epoch 32/100\n",
      " - 3s - loss: 0.2298 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.0579 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04648\n",
      "Epoch 33/100\n",
      " - 3s - loss: 0.1134 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.7978 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.04648\n",
      "Epoch 34/100\n",
      " - 3s - loss: 0.3013 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 1.7658 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04648\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.1971 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 1.7044 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.04648\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.1931 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 1.8598 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.04648\n",
      "Epoch 37/100\n",
      " - 3s - loss: 0.1956 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 1.1922 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.04648\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.1753 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 2.6062 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.04648\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.1280 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 2.0339 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.04648\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.2331 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.5393 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.04648\n",
      "Epoch 41/100\n",
      " - 3s - loss: 0.1282 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.8006 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.04648\n",
      "Epoch 42/100\n",
      " - 3s - loss: 0.1237 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.2377 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.04648\n",
      "Epoch 43/100\n",
      " - 3s - loss: 0.1208 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.5944 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.04648\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.1088 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.2903 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.04648\n",
      "Epoch 45/100\n",
      " - 3s - loss: 0.1121 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.7530 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.04648\n",
      "Epoch 46/100\n",
      " - 3s - loss: 0.1586 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 1.2873 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.04648\n",
      "Epoch 47/100\n",
      " - 3s - loss: 0.1955 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.2661 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.04648\n",
      "Epoch 48/100\n",
      " - 3s - loss: 0.0915 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.7562 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.04648\n",
      "Epoch 49/100\n",
      " - 3s - loss: 0.1810 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 3.3763 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.04648\n",
      "Epoch 50/100\n",
      " - 3s - loss: 0.1267 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 5.1521 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.04648\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.2033 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 1.3669 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.04648\n",
      "Epoch 52/100\n",
      " - 3s - loss: 0.1123 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.1040 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.04648\n",
      "Epoch 53/100\n",
      " - 3s - loss: 0.0983 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.9751 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.04648\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.1089 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.3800 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.04648\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.0484 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.5112 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.04648\n",
      "Epoch 56/100\n",
      " - 3s - loss: 0.1222 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.4276 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.04648\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.1143 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.4658 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.04648\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.0509 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.4157 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.04648\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.0791 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.4446 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.04648\n",
      "Epoch 60/100\n",
      " - 3s - loss: 0.0513 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.3657 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.04648\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.0332 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.7487 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.04648\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.0439 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.4193 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.04648\n",
      "Epoch 63/100\n",
      " - 3s - loss: 0.0895 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.3807 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.04648\n",
      "Epoch 64/100\n",
      " - 3s - loss: 0.0319 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.7047 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00064: val_loss did not improve from 0.04648\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.1082 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.9912 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.04648\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.1101 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.6304 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.04648\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.1738 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.2362 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.04648\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.0418 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2073 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.04648\n",
      "Epoch 69/100\n",
      " - 3s - loss: 0.0455 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.9906 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.04648\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.0542 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.1777 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.04648\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.0379 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.2334 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.04648\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.0365 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.3574 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.04648\n",
      "Epoch 73/100\n",
      " - 3s - loss: 0.0851 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 1.0269 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.04648\n",
      "Epoch 74/100\n",
      " - 3s - loss: 0.0587 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.9556 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.04648\n",
      "Epoch 75/100\n",
      " - 3s - loss: 0.1005 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.4507 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.04648\n",
      "Epoch 76/100\n",
      " - 3s - loss: 0.1268 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.2800 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.04648\n",
      "Epoch 77/100\n",
      " - 3s - loss: 0.2029 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.5246 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.04648\n",
      "Epoch 78/100\n",
      " - 3s - loss: 0.2195 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.7060 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.04648\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.0954 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 2.3371 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.04648\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.1177 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.6039 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.04648\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.1937 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 1.3704 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.04648\n",
      "Epoch 82/100\n",
      " - 3s - loss: 0.1120 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.2639 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.04648\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.1560 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 1.7085 - val_precision: 0.2083 - val_recall: 0.2083 - val_f1: 0.2083 - val_acc: 0.2083\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.04648\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.0857 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.5098 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.04648\n",
      "Epoch 85/100\n",
      " - 4s - loss: 0.1084 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.8750 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.04648\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.0454 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.2359 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.04648\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.0864 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1763 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.04648\n",
      "Epoch 88/100\n",
      " - 3s - loss: 0.0979 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.7176 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.04648\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.0531 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.8208 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.04648\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.0419 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.9019 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.04648\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.0403 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.3629 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.04648\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.1209 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.2864 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.04648\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.0836 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.9448 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.04648\n",
      "Epoch 94/100\n",
      " - 4s - loss: 0.0624 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.7994 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.04648\n",
      "Epoch 95/100\n",
      " - 3s - loss: 0.1002 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 1.1180 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.04648\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.0870 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.3270 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.04648\n",
      "Epoch 97/100\n",
      " - 3s - loss: 0.1028 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.5521 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00097: val_loss did not improve from 0.04648\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.0377 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.8413 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.04648\n",
      "Epoch 99/100\n",
      " - 3s - loss: 0.0632 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 2.8133 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.04648\n",
      "Epoch 100/100\n",
      " - 2s - loss: 0.0818 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.4674 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.04648\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0750 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1350 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13500, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.2099 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.8122 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.13500\n",
      "Epoch 3/100\n",
      " - 3s - loss: 0.2846 - precision: 0.9167 - recall: 0.9167 - f1: 0.9167 - acc: 0.9167 - val_loss: 0.7458 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13500\n",
      "Epoch 4/100\n",
      " - 4s - loss: 0.2466 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 1.2246 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13500\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.1643 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 2.0142 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13500\n",
      "Epoch 6/100\n",
      " - 4s - loss: 0.1220 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 2.6632 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13500\n",
      "Epoch 7/100\n",
      " - 4s - loss: 0.1697 - precision: 0.9083 - recall: 0.9083 - f1: 0.9083 - acc: 0.9083 - val_loss: 2.9120 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13500\n",
      "Epoch 8/100\n",
      " - 4s - loss: 0.1567 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 1.9290 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13500\n",
      "Epoch 9/100\n",
      " - 4s - loss: 0.1096 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.6042 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13500\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.0936 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.8358 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13500\n",
      "Epoch 11/100\n",
      " - 3s - loss: 0.1219 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 1.2137 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13500\n",
      "Epoch 12/100\n",
      " - 3s - loss: 0.0397 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 1.1321 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13500\n",
      "Epoch 13/100\n",
      " - 3s - loss: 0.1406 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 2.1509 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13500\n",
      "Epoch 14/100\n",
      " - 3s - loss: 0.1148 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.6411 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13500\n",
      "Epoch 15/100\n",
      " - 3s - loss: 0.0775 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.3786 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13500\n",
      "Epoch 16/100\n",
      " - 3s - loss: 0.0846 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.3517 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13500\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.1254 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.5002 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13500\n",
      "Epoch 18/100\n",
      " - 3s - loss: 0.1773 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.4334 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13500\n",
      "Epoch 19/100\n",
      " - 3s - loss: 0.1188 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1438 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.13500\n",
      "Epoch 20/100\n",
      " - 3s - loss: 0.1752 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.1160 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.13500 to 0.11602, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 21/100\n",
      " - 3s - loss: 0.0851 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1226 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11602\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.1044 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.1146 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11602 to 0.11460, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.1009 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1952 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.11460\n",
      "Epoch 24/100\n",
      " - 3s - loss: 0.1092 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.1406 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11460\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.0436 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2922 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11460\n",
      "Epoch 26/100\n",
      " - 3s - loss: 0.1173 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.1111 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.11460 to 0.11110, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 27/100\n",
      " - 3s - loss: 0.0284 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1617 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11110\n",
      "Epoch 28/100\n",
      " - 3s - loss: 0.0521 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1234 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11110\n",
      "Epoch 29/100\n",
      " - 3s - loss: 0.0920 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.1091 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.11110 to 0.10914, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 30/100\n",
      " - 3s - loss: 0.2442 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.0291 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.10914 to 0.02906, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 31/100\n",
      " - 3s - loss: 0.1010 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.3365 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02906\n",
      "Epoch 32/100\n",
      " - 3s - loss: 0.2573 - precision: 0.8917 - recall: 0.8917 - f1: 0.8917 - acc: 0.8917 - val_loss: 0.0612 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02906\n",
      "Epoch 33/100\n",
      " - 3s - loss: 0.3150 - precision: 0.8833 - recall: 0.8833 - f1: 0.8833 - acc: 0.8833 - val_loss: 0.2728 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02906\n",
      "Epoch 34/100\n",
      " - 3s - loss: 0.0980 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 1.4755 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02906\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.2128 - precision: 0.9000 - recall: 0.9000 - f1: 0.9000 - acc: 0.9000 - val_loss: 2.6246 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02906\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.2021 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.1902 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02906\n",
      "Epoch 37/100\n",
      " - 3s - loss: 0.0941 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1100 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02906\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.1018 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0030 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.02906 to 0.00304, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.0442 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0167 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00304\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.0701 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.0115 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00304\n",
      "Epoch 41/100\n",
      " - 3s - loss: 0.1313 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.0110 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00304\n",
      "Epoch 42/100\n",
      " - 4s - loss: 0.0852 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 2.4668 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00304\n",
      "Epoch 43/100\n",
      " - 3s - loss: 0.1171 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 6.5915 - val_precision: 0.4167 - val_recall: 0.4167 - val_f1: 0.4167 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00304\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.1609 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 9.1552 - val_precision: 0.3750 - val_recall: 0.3750 - val_f1: 0.3750 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00304\n",
      "Epoch 45/100\n",
      " - 3s - loss: 0.1896 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.8904 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00304\n",
      "Epoch 46/100\n",
      " - 3s - loss: 0.0930 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0223 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00304\n",
      "Epoch 47/100\n",
      " - 3s - loss: 0.1012 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0011 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00304 to 0.00107, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
      "Epoch 48/100\n",
      " - 3s - loss: 0.1808 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.2863 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00107\n",
      "Epoch 49/100\n",
      " - 3s - loss: 0.1103 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.1462 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00107\n",
      "Epoch 50/100\n",
      " - 3s - loss: 0.0887 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0102 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00107\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.0363 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0018 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00107\n",
      "Epoch 52/100\n",
      " - 3s - loss: 0.0739 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.1703 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00107\n",
      "Epoch 53/100\n",
      " - 3s - loss: 0.0369 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1376 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00107\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.0300 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0109 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00107\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.0647 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0017 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00107\n",
      "Epoch 56/100\n",
      " - 3s - loss: 0.0498 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0014 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00107\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.0552 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0160 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00107\n",
      "Epoch 58/100\n",
      " - 4s - loss: 0.0588 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0033 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00107\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.0394 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0028 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00107\n",
      "Epoch 60/100\n",
      " - 3s - loss: 0.0210 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0687 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00107\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.0164 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0026 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      " - 3s - loss: 0.0657 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0219 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00107\n",
      "Epoch 63/100\n",
      " - 3s - loss: 0.0245 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0104 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00107\n",
      "Epoch 64/100\n",
      " - 3s - loss: 0.0220 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0037 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00107\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.0375 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0019 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00107\n",
      "Epoch 66/100\n",
      " - 4s - loss: 0.0214 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0128 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00107\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.0477 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0113 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00107\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.0424 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0239 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00107\n",
      "Epoch 69/100\n",
      " - 3s - loss: 0.0304 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0101 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00107\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.0171 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0078 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00107\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.0397 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0075 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00107\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.0275 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.3058 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00107\n",
      "Epoch 73/100\n",
      " - 3s - loss: 0.0278 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0862 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00107\n",
      "Epoch 74/100\n",
      " - 3s - loss: 0.0578 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0126 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00107\n",
      "Epoch 75/100\n",
      " - 3s - loss: 0.0515 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0338 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00107\n",
      "Epoch 76/100\n",
      " - 3s - loss: 0.0211 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1579 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00107\n",
      "Epoch 77/100\n",
      " - 4s - loss: 0.0467 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.7550 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00107\n",
      "Epoch 78/100\n",
      " - 3s - loss: 0.1642 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 4.1645 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00107\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.0803 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1195 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00107\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.0704 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0194 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00107\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.0691 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.0531 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00107\n",
      "Epoch 82/100\n",
      " - 3s - loss: 0.0321 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1358 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00107\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.0551 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0911 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00107\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.1165 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.1388 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00107\n",
      "Epoch 85/100\n",
      " - 3s - loss: 0.0636 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1297 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00107\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.0700 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0677 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00107\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.1335 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.3131 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00107\n",
      "Epoch 88/100\n",
      " - 3s - loss: 0.0648 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.3526 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00107\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.0422 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.5440 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00107\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.0448 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.3609 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00107\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.0667 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 1.0968 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00107\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.1577 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.0679 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00107\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.1221 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0102 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00107\n",
      "Epoch 94/100\n",
      " - 3s - loss: 0.1422 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.4311 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00107\n",
      "Epoch 95/100\n",
      " - 3s - loss: 0.1078 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.1577 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00107\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.0801 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0036 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00107\n",
      "Epoch 97/100\n",
      " - 3s - loss: 0.0664 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.3017 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00107\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.0340 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0060 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00107\n",
      "Epoch 99/100\n",
      " - 3s - loss: 0.0210 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0289 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00107\n",
      "Epoch 100/100\n",
      " - 3s - loss: 0.0590 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.0065 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00107\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0436 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0026 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00259, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.1561 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.3382 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00259\n",
      "Epoch 3/100\n",
      " - 3s - loss: 0.1770 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.0242 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00259\n",
      "Epoch 4/100\n",
      " - 3s - loss: 0.1900 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.1980 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00259\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.1355 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.3816 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00259\n",
      "Epoch 6/100\n",
      " - 3s - loss: 0.0392 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 3.8444 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00259\n",
      "Epoch 7/100\n",
      " - 3s - loss: 0.0762 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 3.1466 - val_precision: 0.3750 - val_recall: 0.3750 - val_f1: 0.3750 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00259\n",
      "Epoch 8/100\n",
      " - 3s - loss: 0.1212 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 2.2593 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00259\n",
      "Epoch 9/100\n",
      " - 4s - loss: 0.0448 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 2.2673 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00259\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.1148 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 1.2081 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00259\n",
      "Epoch 11/100\n",
      " - 3s - loss: 0.0752 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 1.0173 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00259\n",
      "Epoch 12/100\n",
      " - 3s - loss: 0.0646 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1932 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00259\n",
      "Epoch 13/100\n",
      " - 3s - loss: 0.1395 - precision: 0.9250 - recall: 0.9250 - f1: 0.9250 - acc: 0.9250 - val_loss: 0.0728 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00259\n",
      "Epoch 14/100\n",
      " - 3s - loss: 0.1290 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.0117 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00259\n",
      "Epoch 15/100\n",
      " - 3s - loss: 0.0106 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1062 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00259\n",
      "Epoch 16/100\n",
      " - 3s - loss: 0.1318 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.0752 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00259\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.0295 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.8597 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00259\n",
      "Epoch 18/100\n",
      " - 3s - loss: 0.0917 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.2533 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00259\n",
      "Epoch 19/100\n",
      " - 3s - loss: 0.0895 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.1488 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00259\n",
      "Epoch 20/100\n",
      " - 3s - loss: 0.0340 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.4400 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00259\n",
      "Epoch 21/100\n",
      " - 3s - loss: 0.2055 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.0498 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00259\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.0854 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1490 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00259\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.0582 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2880 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00259\n",
      "Epoch 24/100\n",
      " - 4s - loss: 0.1502 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.1505 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00259\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.0530 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0938 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00259\n",
      "Epoch 26/100\n",
      " - 3s - loss: 0.0824 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0029 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00259\n",
      "Epoch 27/100\n",
      " - 3s - loss: 0.0236 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.2066 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00259\n",
      "Epoch 28/100\n",
      " - 3s - loss: 0.0464 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1082 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00259\n",
      "Epoch 29/100\n",
      " - 3s - loss: 0.0310 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0033 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00259\n",
      "Epoch 30/100\n",
      " - 3s - loss: 0.0173 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0926 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00259\n",
      "Epoch 31/100\n",
      " - 3s - loss: 0.0195 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1327 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00259\n",
      "Epoch 32/100\n",
      " - 3s - loss: 0.0147 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1078 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00259\n",
      "Epoch 33/100\n",
      " - 3s - loss: 0.0154 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0404 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00259\n",
      "Epoch 34/100\n",
      " - 3s - loss: 0.0360 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0365 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00259\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.0283 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0382 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00259\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.0113 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.6984 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00259\n",
      "Epoch 37/100\n",
      " - 3s - loss: 0.0479 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0568 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00259\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.0522 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0288 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00259\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.1030 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0270 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00259\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.1246 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.9608 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00259\n",
      "Epoch 41/100\n",
      " - 4s - loss: 0.1055 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.4441 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00259\n",
      "Epoch 42/100\n",
      " - 3s - loss: 0.2062 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.4301 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00259\n",
      "Epoch 43/100\n",
      " - 3s - loss: 0.0826 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0474 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00259\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.0900 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.4642 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00259\n",
      "Epoch 45/100\n",
      " - 4s - loss: 0.1894 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.0428 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00259\n",
      "Epoch 46/100\n",
      " - 3s - loss: 0.0364 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.9240 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00259\n",
      "Epoch 47/100\n",
      " - 3s - loss: 0.0928 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.5450 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00259\n",
      "Epoch 48/100\n",
      " - 3s - loss: 0.0198 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.4176 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00259\n",
      "Epoch 49/100\n",
      " - 3s - loss: 0.0497 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.4172 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00259\n",
      "Epoch 50/100\n",
      " - 3s - loss: 0.0329 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2899 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00259\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.0168 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.7140 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00259\n",
      "Epoch 52/100\n",
      " - 3s - loss: 0.0504 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.4795 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00259\n",
      "Epoch 53/100\n",
      " - 3s - loss: 0.0731 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.2572 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00259\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.0259 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.4813 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00259\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.0185 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.2411 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00259\n",
      "Epoch 56/100\n",
      " - 4s - loss: 0.0311 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1817 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00259\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.0848 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.3008 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00259\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.0844 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.3727 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00259\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.0193 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1751 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00259\n",
      "Epoch 60/100\n",
      " - 4s - loss: 0.0249 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1418 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00259\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.0291 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2399 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00259\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.0202 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.2274 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00259\n",
      "Epoch 63/100\n",
      " - 3s - loss: 0.0175 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0172 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00259\n",
      "Epoch 64/100\n",
      " - 3s - loss: 0.0137 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0072 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00259\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.0086 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0659 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00259\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.0110 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0744 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00259\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.0244 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.2248 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00259\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.0601 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.0885 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00259\n",
      "Epoch 69/100\n",
      " - 3s - loss: 0.0190 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0093 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00259\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.0223 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1039 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00259\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.0069 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0130 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00259\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.1427 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0818 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00259\n",
      "Epoch 73/100\n",
      " - 3s - loss: 0.1039 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0197 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00259\n",
      "Epoch 74/100\n",
      " - 3s - loss: 0.0415 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0399 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00259\n",
      "Epoch 75/100\n",
      " - 3s - loss: 0.0530 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.5808 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00259\n",
      "Epoch 76/100\n",
      " - 3s - loss: 0.0687 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1657 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00259\n",
      "Epoch 77/100\n",
      " - 3s - loss: 0.1214 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 1.6164 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00259\n",
      "Epoch 78/100\n",
      " - 3s - loss: 0.1004 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.1497 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00259\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.0909 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.3662 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00259\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.1319 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.3584 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00259\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.0946 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 1.1482 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00259\n",
      "Epoch 82/100\n",
      " - 3s - loss: 0.1595 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.0520 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00259\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.0569 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0066 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00259\n",
      "Epoch 84/100\n",
      " - 4s - loss: 0.0372 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 2.3963e-04 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00259 to 0.00024, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
      "Epoch 85/100\n",
      " - 3s - loss: 0.0383 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0175 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00024\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.0704 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1407 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00024\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.1271 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.3216 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00024\n",
      "Epoch 88/100\n",
      " - 3s - loss: 0.0490 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.3054 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00024\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.0248 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0055 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00024\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.0292 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0548 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00024\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.0164 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0537 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00024\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.0330 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0740 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00024\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.0375 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0014 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00024\n",
      "Epoch 94/100\n",
      " - 3s - loss: 0.0080 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0081 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00024\n",
      "Epoch 95/100\n",
      " - 3s - loss: 0.0096 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0105 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00024\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.0644 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0129 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00024\n",
      "Epoch 97/100\n",
      " - 4s - loss: 0.0142 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0041 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00024\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.0323 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0142 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00024\n",
      "Epoch 99/100\n",
      " - 3s - loss: 0.2338 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1298 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00024\n",
      "Epoch 100/100\n",
      " - 3s - loss: 0.0495 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0046 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00024\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0721 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.2149 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21493, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.1046 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.1361 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21493 to 0.13608, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0809 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.2187 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13608\n",
      "Epoch 4/100\n",
      " - 3s - loss: 0.0457 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 1.5216 - val_precision: 0.3333 - val_recall: 0.3333 - val_f1: 0.3333 - val_acc: 0.3333\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13608\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.1223 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.7243 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13608\n",
      "Epoch 6/100\n",
      " - 3s - loss: 0.0381 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.4702 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13608\n",
      "Epoch 7/100\n",
      " - 3s - loss: 0.1244 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.2783 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13608\n",
      "Epoch 8/100\n",
      " - 4s - loss: 0.0710 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1857 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13608\n",
      "Epoch 9/100\n",
      " - 3s - loss: 0.0293 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1532 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13608\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.0664 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1253 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.13608 to 0.12531, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 11/100\n",
      " - 3s - loss: 0.0128 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0380 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12531 to 0.03802, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 12/100\n",
      " - 3s - loss: 0.0351 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0319 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03802 to 0.03192, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 13/100\n",
      " - 3s - loss: 0.0307 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0377 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03192\n",
      "Epoch 14/100\n",
      " - 3s - loss: 0.0636 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0672 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03192\n",
      "Epoch 15/100\n",
      " - 3s - loss: 0.0573 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0870 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03192\n",
      "Epoch 16/100\n",
      " - 3s - loss: 0.0783 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0742 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03192\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.0194 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1429 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03192\n",
      "Epoch 18/100\n",
      " - 3s - loss: 0.0950 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.0140 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03192 to 0.01398, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 19/100\n",
      " - 3s - loss: 0.1122 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0569 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01398\n",
      "Epoch 20/100\n",
      " - 3s - loss: 0.0577 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1916 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01398\n",
      "Epoch 21/100\n",
      " - 4s - loss: 0.0542 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2108 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01398\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.1711 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.3213 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01398\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.1125 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.1351 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01398\n",
      "Epoch 24/100\n",
      " - 3s - loss: 0.0287 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.3339 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01398\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.0088 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0154 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01398\n",
      "Epoch 26/100\n",
      " - 3s - loss: 0.1301 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.0393 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01398\n",
      "Epoch 27/100\n",
      " - 3s - loss: 0.0275 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.3560 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01398\n",
      "Epoch 28/100\n",
      " - 3s - loss: 0.0283 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0546 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01398\n",
      "Epoch 29/100\n",
      " - 4s - loss: 0.0905 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.0979 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01398\n",
      "Epoch 30/100\n",
      " - 4s - loss: 0.1039 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.3039 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01398\n",
      "Epoch 31/100\n",
      " - 3s - loss: 0.0618 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0133 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01398 to 0.01333, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 32/100\n",
      " - 3s - loss: 0.0524 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0412 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01333\n",
      "Epoch 33/100\n",
      " - 3s - loss: 0.0251 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0876 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01333\n",
      "Epoch 34/100\n",
      " - 3s - loss: 0.0922 - precision: 0.9500 - recall: 0.9500 - f1: 0.9500 - acc: 0.9500 - val_loss: 0.0602 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01333\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.1622 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.0591 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01333\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.1066 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.0171 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01333\n",
      "Epoch 37/100\n",
      " - 4s - loss: 0.0773 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.6388 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01333\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.1014 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.2571 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01333\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.1859 - precision: 0.9333 - recall: 0.9333 - f1: 0.9333 - acc: 0.9333 - val_loss: 0.1641 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01333\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.0418 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 1.4893 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01333\n",
      "Epoch 41/100\n",
      " - 3s - loss: 0.0372 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 1.0624 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01333\n",
      "Epoch 42/100\n",
      " - 3s - loss: 0.0867 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.6345 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01333\n",
      "Epoch 43/100\n",
      " - 3s - loss: 0.0969 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0963 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01333\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.0409 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1281 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01333\n",
      "Epoch 45/100\n",
      " - 4s - loss: 0.0830 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.4403 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01333\n",
      "Epoch 46/100\n",
      " - 3s - loss: 0.0908 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.6970 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01333\n",
      "Epoch 47/100\n",
      " - 3s - loss: 0.0629 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.2609 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01333\n",
      "Epoch 48/100\n",
      " - 3s - loss: 0.0199 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0634 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01333\n",
      "Epoch 49/100\n",
      " - 3s - loss: 0.0217 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0757 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01333\n",
      "Epoch 50/100\n",
      " - 3s - loss: 0.1093 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 0.1433 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01333\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.0272 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1170 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01333\n",
      "Epoch 52/100\n",
      " - 3s - loss: 0.0270 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1518 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01333\n",
      "Epoch 53/100\n",
      " - 3s - loss: 0.1296 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.0240 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01333\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.0130 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0391 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01333\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.0230 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0303 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01333\n",
      "Epoch 56/100\n",
      " - 3s - loss: 0.0514 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.0390 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01333\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.0400 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0166 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01333\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.0301 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0130 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00058: val_loss improved from 0.01333 to 0.01297, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.0197 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0067 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.01297 to 0.00667, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 60/100\n",
      " - 3s - loss: 0.0284 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0139 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00667\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.0145 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0037 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00667 to 0.00367, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.0057 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0044 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00367\n",
      "Epoch 63/100\n",
      " - 3s - loss: 0.0300 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0088 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00367\n",
      "Epoch 64/100\n",
      " - 4s - loss: 0.0081 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0135 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00367\n",
      "Epoch 65/100\n",
      " - 4s - loss: 0.0148 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0177 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00367\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.0092 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0305 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00367\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.0219 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0293 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00367\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.0435 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0263 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00367\n",
      "Epoch 69/100\n",
      " - 3s - loss: 0.0561 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0261 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00367\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.0123 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0219 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00367\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.0236 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0532 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00367\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.0199 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.0100 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00367\n",
      "Epoch 73/100\n",
      " - 3s - loss: 0.0162 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.3160 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00367\n",
      "Epoch 74/100\n",
      " - 3s - loss: 0.0142 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.0047 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00367\n",
      "Epoch 75/100\n",
      " - 3s - loss: 0.0493 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1294 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00367\n",
      "Epoch 76/100\n",
      " - 4s - loss: 0.0111 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.1898 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00367\n",
      "Epoch 77/100\n",
      " - 3s - loss: 0.1079 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.4031 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00367\n",
      "Epoch 78/100\n",
      " - 3s - loss: 0.0250 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 2.7499 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00367\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.1071 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 2.1701 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00367\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.1548 - precision: 0.9417 - recall: 0.9417 - f1: 0.9417 - acc: 0.9417 - val_loss: 3.2481 - val_precision: 0.5000 - val_recall: 0.5000 - val_f1: 0.5000 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00367\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.0131 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.4590 - val_precision: 0.7917 - val_recall: 0.7917 - val_f1: 0.7917 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00367\n",
      "Epoch 82/100\n",
      " - 3s - loss: 0.0616 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.0357 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00367\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.0718 - precision: 0.9667 - recall: 0.9667 - f1: 0.9667 - acc: 0.9667 - val_loss: 0.6988 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00367\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.0737 - precision: 0.9583 - recall: 0.9583 - f1: 0.9583 - acc: 0.9583 - val_loss: 0.1311 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00367\n",
      "Epoch 85/100\n",
      " - 3s - loss: 0.0634 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.5299 - val_precision: 0.6667 - val_recall: 0.6667 - val_f1: 0.6667 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00367\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.0472 - precision: 0.9750 - recall: 0.9750 - f1: 0.9750 - acc: 0.9750 - val_loss: 0.3187 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00367\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.0435 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2669 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00367\n",
      "Epoch 88/100\n",
      " - 4s - loss: 0.0569 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1128 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00367\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.0407 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1404 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00367\n",
      "Epoch 90/100\n",
      " - 4s - loss: 0.0430 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2294 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00367\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.0248 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1104 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00367\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.0797 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.1281 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00367\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.0311 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0494 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00367\n",
      "Epoch 94/100\n",
      " - 4s - loss: 0.0257 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0656 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00367\n",
      "Epoch 95/100\n",
      " - 3s - loss: 0.0119 - precision: 1.0000 - recall: 1.0000 - f1: 1.0000 - acc: 1.0000 - val_loss: 0.3647 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00367\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.0335 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.6435 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00367\n",
      "Epoch 97/100\n",
      " - 3s - loss: 0.0491 - precision: 0.9917 - recall: 0.9917 - f1: 0.9917 - acc: 0.9917 - val_loss: 0.1541 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00367\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.0665 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2936 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00367\n",
      "Epoch 99/100\n",
      " - 3s - loss: 0.0481 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.2307 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00367\n",
      "Epoch 100/100\n",
      " - 3s - loss: 0.0802 - precision: 0.9833 - recall: 0.9833 - f1: 0.9833 - acc: 0.9833 - val_loss: 0.0176 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00367\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n",
    "\n",
    "for ix, (train_index, test_index) in enumerate(kf.split(range(len(dataset.split_train_test(\"train\")[0])))):\n",
    "                                               \n",
    "    tg = DATASET(SHAPE, BATCH_SIZE, train_index, BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=True)\n",
    "    vg = DATASET(SHAPE, BATCH_SIZE, test_index , BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=False)\n",
    "        \n",
    "    schedule = SGDRScheduler(min_lr=1e-6,\n",
    "                             max_lr=1e-3,\n",
    "                             steps_per_epoch=np.ceil(EPOCHS/BATCH_SIZE),\n",
    "                             lr_decay=0.9,\n",
    "                             cycle_length=10,\n",
    "                             mult_factor=2.)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[precision, recall, f1, 'acc'])\n",
    "\n",
    "    model_ckpt = \"BRAIN_TUMOR_FOLD_\"+str(ix)+\".h5\"\n",
    "    callbacks = [ModelCheckpoint(model_ckpt, monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False),\n",
    "                 TensorBoard(log_dir='./log_'+str(ix), update_freq='batch'), \n",
    "                 schedule] \n",
    "                                               \n",
    "    model.fit_generator(tg.data_generator(),\n",
    "                        steps_per_epoch=len(train_index)//BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=2,\n",
    "                        validation_data=vg.data_generator(),\n",
    "                        validation_steps=len(test_index)//BATCH_SIZE,\n",
    "                        callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    gen = DATASET(SHAPE, BATCH_SIZE, range(1), BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=False).split_train_test(\"test\")\n",
    "                       \n",
    "    x = np.empty((len(gen[0]),)+SHAPE, dtype=np.float32)\n",
    "    y = np.empty((len(gen[1]), 2), dtype=np.float32)\n",
    "    \n",
    "    for ix, path in tqdm(enumerate(gen[0])):\n",
    "        img = np.array(Image.open(gen[0][ix]))\n",
    "        img = resize(img, SHAPE)\n",
    "\n",
    "        label = gen[1][ix]\n",
    "\n",
    "        x[ix] = img\n",
    "        y[ix] = label\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [00:01, 55.91it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Threshold predictions with THRESH_VAL\n",
    "def threshold_arr(array):\n",
    "    # Get all value from array\n",
    "    # Compare calue with THRESH_VAL \n",
    "    # IF value >= THRESH_VAL. round to 1\n",
    "    # ELSE. round to 0\n",
    "    new_arr = []\n",
    "    for ix, val in enumerate(array):\n",
    "        loc = np.array(val).argmax(axis=0)\n",
    "        k = list(np.zeros((len(val)), dtype=np.float32))\n",
    "        k[loc]=1\n",
    "        new_arr.append(k)\n",
    "        \n",
    "    return np.array(new_arr, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5898145374498869, 0.8684210557686655, 0.8684210557686655, 0.8684209961640207, 0.8684210557686655]\n",
      "[0.639385041437651, 0.7763157926107708, 0.7763157926107708, 0.776315733006126, 0.7763157926107708]\n",
      "[0.5551100843831113, 0.9078947399791918, 0.9078947399791918, 0.907894680374547, 0.9078947399791918]\n",
      "[0.44534744714435776, 0.9210526284418608, 0.9210526284418608, 0.921052568837216, 0.9210526284418608]\n",
      "[0.2933201099696912, 0.9473684241897181, 0.9473684241897181, 0.9473683645850733, 0.9473684241897181]\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    model = load_model(\"BRAIN_TUMOR_FOLD_{}.h5\".format(i), custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n",
    "    print(model.evaluate(x, y, verbose=0))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig(\"confusion matrix_best.jpg\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "y_pred = models[4].predict(x, verbose=0)\n",
    "y_proba = []\n",
    "for ix, i in enumerate(y_pred.argmax(axis=1)):\n",
    "    if i==0:\n",
    "        y_proba.append(1-y_pred[ix][i])\n",
    "    else:\n",
    "        y_proba.append(y_pred[ix][i])\n",
    "        \n",
    "fpr, tpr, thresholds = roc_curve(y.argmax(axis=1), y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, label='Brain Tumor Classification (AUC = {})'.format(round(roc_auc,3)))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"ROC_best.jpg\", dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BREAK_HIST_GANGSTERS_v1.0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
